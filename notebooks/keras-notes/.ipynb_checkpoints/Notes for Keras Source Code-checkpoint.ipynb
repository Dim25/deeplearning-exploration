{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for Keras (1.0.6) Source Code\n",
    "\n",
    "## why\n",
    "- Notes taken when I am trying to understand the code\n",
    "- Provides very high-level snapshot of the source code structure\n",
    "- Serves as roadmap for future references\n",
    "\n",
    "## what\n",
    "- Covers core packages so far, namely, `backend`, `engine`, `layers`, `models`, `optimizers`, `objectives`\n",
    "- Big picture about the packages, taking LEGO as an analogy\n",
    "    - `backend` defines LEGO blocks of different shapes. You can build combined blocks by connecting them in different ways. It's the foundation of other packages.\n",
    "    - `engine`, in its two subpackage `topology` and `training`, defines two most important classes - `Layer` and `Model` classes. These are the parents of all other layers and models living in Keras.\n",
    "    - `layers` and `models` packages contain all kinds of templates, which are similiar to the LEGO \"recipes\". Of course you can define your own. Just make sure to stick with the API so it works with other parts of Keras.\n",
    "    - finally, `optimizers` package with others e.g., `objectives`, `regularizers`, `metrics` make your LEGO building alive.\n",
    "    - It is really fun to see how the code organization in Keras encourages **open to extensions and close to modifications** design.\n",
    "    \n",
    "## how\n",
    "- There might be mistakes and typos, because it is just my understanding as a learnner. Please bear this in mind if you read it.\n",
    "- I found it really helpful to my understanding when I look at the test code first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. conventions\n",
    "- shapes and dtypes are the most important specs for layers and variables in Keras. Their general meanings are,\n",
    "    - `batch_input_shape`: the most general shape information, its first element should always be the _batch_size_, followed by the dimensions of actual inputs. It is useful in some models (e.g., stateful RNN) where batch_size needs to be explicitly spelled\n",
    "    - `input_shape`: specifies the shape of input/output, without a `batch_size` (or = None). Common data used in Keras include,\n",
    "        - flat vectors, so `input_shape = (, vec_dim)`, e.g., for Dense\n",
    "        - sequence of integers, so `input_shape = (, max_int)`, e.g., for Embedding\n",
    "        - sequence of vectors (e.g., sentence), so `input_shape = (nwords, word_vec_dim)`, e.g., for LSTM\n",
    "        - images (in theano format), so `input_shape = (ncolors, width, height)`,e.g., for Conv2D\n",
    "        - sequence of images (videos)? ,,,\n",
    "    - `input_dim` is just a shortcut variable to specify `input_shape` is a single number. If the `input_shape` is a tuple of 2, it is usually specified by `input_dim` and `input_length` (sequence length) \n",
    "- most of time you just need to specify input shape information in the first layer (input layer) of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. `backend` package\n",
    "- in keras/backend folder, including \n",
    "    - `__init__.py`: exposes common interface for backend variables and operations\n",
    "    - `common.py`: utility functions common for both tf and th backends \n",
    "    - `tensorflow_backend.py`: tf implementation of the common interface\n",
    "    - `theano_backend.py`: th implementation\n",
    "- the common interface implemented in tf and th backend are mostly\n",
    "    - setter/getter of singleton varaibles, e.g., `_LEARNING_PHASE`, `_SESSION`, most of which are private (hidden from interface to users)\n",
    "    - factory methods of `tensors`. There are roughly two types of tensors in tf/th libraries, which are exposed in Keras\n",
    "        - `variable tensor`: tensors that you might have a value for. The value will be the state of the tensor, e.g., weights in your model. **Those are usually used as states of your model.**\n",
    "        - `placeholder tensor`: tensors that you only have information of shapes about. Examples are training batches as they will be changed at each iteration - you don't have to maintain their states, you just need their shapes when building the model. **Those are usually used as inputs to your model.**\n",
    "    - operators: the way that you can combine to generate new tensors on existing ones, e.g., gradient, linear-algebra, shape-manipulation and even rnn.\n",
    "- Keras `backend` package summarizes very well what to expect in lower-level deep learning libraries: _it is all about tensors and their computation dependencies._\n",
    "- One attractive feature of Keras is to directly use the backend API (K api). Because the layers and models expose both an object-oriented and functional interface, it is very intuitive to shift from the two levels. One good example is Keras's [neural_style_transfer](https://github.com/fchollet/keras/blob/master/examples/neural_style_transfer.py) code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. `engine.topology` package\n",
    "\n",
    "### 2.1 `Layer` and `Node`\n",
    "- teh package conceptually defines how the computation flows in Keras, specially via `Node` and `Layer` classes.\n",
    "- if you have ever read about [how simple a layer is implemented in theano](http://deeplearning.net/tutorial/mlp.html#mlp), you might first find it surprising why so much code were put in Keras layers implementations.\n",
    "- like most objects in Keras, layers are implemented as *functors - callable functions (with \\__call\\__ defined) with attributes*. It is like a mix of object-oriented and functional style. The OO part allows functors to have encapulation of states, and hierarchy of inheritances. The functional part makes it easy to be piped together and used (as \"verbs\") with other objects such as tensors (data/weights) etc.\n",
    "- As such, a lot of code goes into maintaining, checking the attributes of layers (e.g., its input, output shape and variables), as well as making them connected in a flexible way.\n",
    "- In Keras, layers are connected via \"nodes\". An illustration below. This is implmented in `Layer.add_inbound_node()` in `keras/engine/topology.py`\n",
    "![Keras Layer Connection](keras-layers.png)\n",
    "- In summary\n",
    "    - nodes are connectors of layers\n",
    "    - one node can have mulitple inbound layers and a single outbound layer\n",
    "    - another note to take is that, even though conceptually the input and output are attached to layers, they are actually implemented attributes of the inbound nodes of the layer, instead of the layer itself (e.g., see `input()` and `output()` property of the `Layer` class). Layers are just code that map inputs to output.\n",
    "    - to make this clearer, in the documentation to the `Node` class, there is a line for verification of every node:\n",
    "    ```\n",
    "    input_tensors[i] == \n",
    "    inbound_layers[i].inbound_nodes[node_indices[i]].output_tensors[tensor_indices[i]]\n",
    "    ```\n",
    "This effectively says that *the ith input tensor to a node is its ith input layer's output, which is in turn represented as the layer's jth inbound node's kth output tensor*, where j = node_indices[i] and k = tensor_indices[i]\n",
    "    - tensor variables will have extra attributes, e.g., `_keras_shape`, `_users_learning_phase` and `_keras_history` when they are added to the nodes/layers in Keras. These attrs will facilitate shape inference and other computations.\n",
    "\n",
    "### 2.2 What does a layer need to function properly?\n",
    "\n",
    "### 2.3 Write your own Keras layers\n",
    "- [documentation](https://keras.io/layers/writing-your-own-keras-layers/) and [simple example](https://github.com/fchollet/keras/blob/master/examples/antirectifier.py)\n",
    "- with help of code in Layer class, it boils down to implementing 4 functions\n",
    "    - `__init__(self, output_dim, **kwargs)`\n",
    "        - it is OK to ignore it if your layer doesn't have extra states to initialize, so `Layer.__init__()` will be automatically called.\n",
    "        - if you implement it, make sure to call `super(MyLayer,self).__init__(**kwargs)` otherwise the inner mechanism won't work.\n",
    "    - `build(self, input_shape)`\n",
    "    - `call(self, x, mask = None)`\n",
    "    - `get_output_shape_for(self, input_shape)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
