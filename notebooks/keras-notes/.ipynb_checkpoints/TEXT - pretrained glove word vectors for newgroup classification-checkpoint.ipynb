{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- [keras example](https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py)\n",
    "- [how word-embedding and image-vectors used to train VQA](http://localhost:8888/notebooks/USECASE%20-%20Visual%20Q%20%26%20A%20by%20Keras.ipynb)\n",
    "- [Glove pretrained word vectors](http://nlp.stanford.edu/projects/glove/)\n",
    "- [20 newgroup classification](http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html)\n",
    "\n",
    "## Structure\n",
    "- Use word-embedding layer as the first layer for downstream text tasks (e.g., classification)\n",
    "- Set word-embedding layer weights with pretrained Glove vectors, and freeze it by setting `trainable = False`\n",
    "- Use 1D CNN to convert each article into a single vector (level by level) - usually with comparable performance but much faster to train than LSTM\n",
    "- Do traditional classification by a dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980M (CNMeM is disabled, cuDNN 5005)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.0.6'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert texts into hash sequences\n",
    "- it is de-facto for deep learning text applications to convert them into sequence of indices. These indices are hash keys to vocabulary.\n",
    "- the output shape should be a 2D tensor, (n_articles, max_words_per_article). For some models the number of words for each sequence may not be necessarily the same, but most of time they need to be padded/truncated to a fixed length, specially for sentences or short articles.\n",
    "- After word embedding, the above sequence matrix will be converted to 3D matrix (n_articles, max_words_per_article, word_vec_dim).\n",
    "- the word vectors can be 1-hot-encoding (or tfidf), or word embedding, depending on what layer is used as the first in the model\n",
    "- `keras.preprocessing.text` and `keras.preprocessing.image` are specially implemented to faciliate those tasks. And the starting point is usually a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19997 19997\n"
     ]
    }
   ],
   "source": [
    "## load news group - they are plain files distributed in different group folders\n",
    "def read_raw_data(data_folder):\n",
    "    texts, labels = [], []\n",
    "    for f in glob(data_folder + \"/*/*\"):\n",
    "        label = f.split(\"/\")[-2]\n",
    "        text= open(f).read()\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "    return texts, labels\n",
    "        \n",
    "texts, labels = read_raw_data(\"../../data/newsgroup_20/\")\n",
    "print len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## parameters for preprocessing and modelling\n",
    "params = {\n",
    "    \"vocab_size\": 20000 # it is common to cache only 20,000 words, and filter out infrequent\n",
    "    , \"max_seq_len\": 1000 # fix the # of words in each article by padding or trucating\n",
    "    , \"word_dim\": 100 # dimensionality of word space, 300 for maximum performance, 100 for small dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19997 333\n"
     ]
    }
   ],
   "source": [
    "## Things start with a tokenizer for text data preprocessing\n",
    "tokenizer = text.Tokenizer(nb_words=params[\"vocab_size\"], )\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "print len(sequences), len(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214909 214909\n",
      "min and max word index: 1 214909\n",
      "Notice keras needs word index built from 1 to vocab_size, 0 is reserved for PADDING\n",
      "Does it make more sense for Keras API to make len(tokenizer.word_index) == max_len?\n"
     ]
    }
   ],
   "source": [
    "## vocabulary are essentially two structures:\n",
    "## word2index and index2word\n",
    "word2index = tokenizer.word_index\n",
    "index2word = dict([(v,k) for k,v in word2index.items()])\n",
    "print len(word2index), len(index2word)\n",
    "print \"min and max word index:\", min(index2word.keys()), max(index2word.keys())\n",
    "print \"Notice keras needs word index built from 1 to vocab_size, 0 is reserved for PADDING\"\n",
    "print \"Does it make more sense for Keras API to make len(tokenizer.word_index) == max_len?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19997, 1000)\n"
     ]
    }
   ],
   "source": [
    "## we almost have everything for a valid text input to keras model\n",
    "## the only thing left is to pad/truncated sequences to make them of fixed size\n",
    "X = sequence.pad_sequences(sequences, maxlen=params[\"max_seq_len\"])\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19997, 20)\n"
     ]
    }
   ],
   "source": [
    "## we do the same thing to labels, notice keras needs 1-hot encoding for multi-classification\n",
    "unique_labels = list(set(labels))\n",
    "index2label = dict(enumerate(unique_labels))\n",
    "label2index = dict([(v,k) for k,v in index2label.items()])\n",
    "encoded_labels = map(label2index.get, labels)\n",
    "Y = np_utils.to_categorical(encoded_labels)\n",
    "print Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15997, 1000), (4000, 1000)]\n",
      "[(15997, 20), (4000, 20)]\n"
     ]
    }
   ],
   "source": [
    "## to make it fair, we also need to keep a separate test (and also validation for training)\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(X, Y, test_size = 0.2)\n",
    "\n",
    "print map(lambda x: x.shape, [train_X, valid_X])\n",
    "print map(lambda x: x.shape, [train_Y, valid_Y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the word vectors from GLove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 100\n"
     ]
    }
   ],
   "source": [
    "## load embedding weights from GLOVE vectors\n",
    "## Glove vector file format can be read from\n",
    "## https://spacy.io/docs/tutorials/load-new-word-vectors\n",
    "def load_glove_vecs(fpath):\n",
    "    word2vec = {}\n",
    "    for line in open(fpath).readlines():\n",
    "        word, vec = line.split(\" \", 1)\n",
    "        word2vec[word] = np.asarray(vec.split(\" \"), dtype=\"float32\")\n",
    "    return word2vec\n",
    "word2vec = load_glove_vecs(\"../../data/glove/glove.6B.100d.txt\")\n",
    "print len(word2vec), len(word2vec.values()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## the number of words, and thus the number of word vectors\n",
    "## is the min of max_nb_words in vocab and the actual unique number of words from texts\n",
    "nb_words = min(params[\"vocab_size\"], len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20001, 100)\n",
      "Here instead of storing the whole vector set for word_index, we only store those up to vocab_size\n"
     ]
    }
   ],
   "source": [
    "## build embedding vector weights for vocabulary\n",
    "## unknown and padding words will be represented as all-zeros\n",
    "\n",
    "## vocab_vecs are of shape (vocab_size+1, word_dim)\n",
    "## +1 for PADDING index 0\n",
    "vocab_vecs = np.zeros((nb_words+1, params[\"word_dim\"]), dtype=\"float32\")\n",
    "for i in xrange(1, params[\"vocab_size\"]+1):\n",
    "    if i > nb_words: continue\n",
    "    word = index2word[i]\n",
    "    if word in word2vec:\n",
    "        vocab_vecs[i] = word2vec[word]\n",
    "print vocab_vecs.shape\n",
    "print \"Here instead of storing the whole vector set for word_index,\",\n",
    "print \"we only store those up to vocab_size\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## index sequences from text - model input (batch_size, seq_len)\n",
    "v_seqs = layers.Input(shape = (params[\"max_seq_len\"], ), dtype = \"int32\", name=\"v_seqs\")\n",
    "\n",
    "## embedding layer and embedded seq vector - (batch_size, seq_len, word_dim)\n",
    "## set trainalbe = False to fix the word vector representation\n",
    "## weights of a layer is a list of np.arrays (each for a parameter, e.g., W, b) \n",
    "layer_embedding = layers.Embedding(input_dim=nb_words+1, \n",
    "                                   input_length=params[\"max_seq_len\"], \n",
    "                                   output_dim=params[\"word_dim\"],\n",
    "                                   weights = [vocab_vecs],\n",
    "                                   trainable = False,\n",
    "                                   name = \"layer_embedding\")\n",
    "v_embedded_seqs = layer_embedding(v_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity between king-man+woman and queen 0.783441347372\n"
     ]
    }
   ],
   "source": [
    "## test with the embeding layer and output\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "seq_to_vec = K.function(inputs = [v_seqs], outputs=[v_embedded_seqs])\n",
    "\n",
    "def get_word_vectors(word):\n",
    "    seqs = sequence.pad_sequences( np.array([[word2index[word]]]), \n",
    "                                  maxlen=params[\"max_seq_len\"], )\n",
    "    vec = seq_to_vec([seqs])[0][0, -1, :]\n",
    "    return vec\n",
    "\n",
    "king = get_word_vectors(\"king\")\n",
    "man = get_word_vectors(\"man\")\n",
    "woman = get_word_vectors(\"woman\")\n",
    "queen = get_word_vectors(\"queen\")\n",
    "\n",
    "queen_to_be = king - man + woman\n",
    "\n",
    "print \"cosine similarity between king-man+woman and queen\", 1 - cosine(queen, queen_to_be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## use 1D cnn to convert sequence vectors into single vectors\n",
    "## 5 is usually a magic number for filter_size for both texts and images\n",
    "## ususally more cnn layers (conv + maxpool) give better results\n",
    "## use dropouts if necessary to prevent overfitting\n",
    "v_seq_vectors = layers.Conv1D(128, 5, activation=\"relu\")(v_embedded_seqs)\n",
    "v_seq_vectors = layers.MaxPooling1D(5)(v_seq_vectors)\n",
    "v_seq_vectors = layers.Conv1D(128, 5, activation=\"relu\")(v_embedded_seqs)\n",
    "v_seq_vectors = layers.MaxPooling1D(5)(v_seq_vectors)\n",
    "v_seq_vectors = layers.Conv1D(128, 5, activation=\"relu\")(v_embedded_seqs)\n",
    "v_seq_vectors = layers.MaxPooling1D(35)(v_seq_vectors)\n",
    "v_seq_vectors = layers.Dropout(.5)(v_seq_vectors)\n",
    "\n",
    "## flatten and dense layer for classification\n",
    "v_article_vectors = layers.Flatten()(v_seq_vectors)\n",
    "v_hidden_vectors = layers.Dense(128, activation=\"relu\")(v_article_vectors)\n",
    "v_probs = layers.Dense(len(label2index), activation=\"softmax\")(v_hidden_vectors)\n",
    "\n",
    "model = models.Model(input = v_seqs, output=v_probs)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15997 samples, validate on 4000 samples\n",
      "Epoch 1/2\n",
      "15997/15997 [==============================] - 10s - loss: 1.4324 - acc: 0.5495 - val_loss: 0.2558 - val_acc: 0.9330\n",
      "Epoch 2/2\n",
      "15997/15997 [==============================] - 10s - loss: 0.2703 - acc: 0.9186 - val_loss: 0.1782 - val_acc: 0.9415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2bb57226d0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## should give 95% accuracy after 2 epoches\n",
    "model.fit(train_X, train_Y, nb_epoch=2, validation_data=(valid_X, valid_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
