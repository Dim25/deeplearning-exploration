{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [char-nn by Karpathy](https://github.com/karpathy/char-rnn)\n",
    "- [RNN on WILDML](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/) and [code](https://github.com/dennybritz/rnn-tutorial-rnnlm)\n",
    "- implement simple RNN (nor LSTM nor GRU) with theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980M (CNMeM is disabled, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import shared, function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theano.config.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate some simple artificial data to better understand the mechanism of RNN\n",
    "- Using [The Adventures of Tom Sawyer](https://www.gutenberg.org/files/74/74.txt) from gutenberg project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 421909\n"
     ]
    }
   ],
   "source": [
    "texts = open(\"../data/TheAdventuresofTomSawyer.txt\").read()\n",
    "charset = set(texts)\n",
    "ind2char = dict(enumerate(charset))\n",
    "char2ind = dict(map(reversed, ind2char.items()))\n",
    "data = map(char2ind.get, texts)\n",
    "\n",
    "print len(charset), len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## char RNN with explicit loop\n",
    "- fixed window lenght\n",
    "- Because the network is not really deep (max seq len is 5), so using simple weights initialization\n",
    "- Use traditional `tanh` for linearity\n",
    "- simple derivative clip between -5, 5\n",
    "- reset hidden state vector to 0 every time after going through the seqs of the whole dataset - we might use a different strategy for sentence modelling, see below for differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "    def __init__(self, L, D, H, lr, lmbda = 0):\n",
    "        \"\"\"\n",
    "        L: sequence lenght, D: vocabulary size (word dimension)\n",
    "        H: hidden size\n",
    "        lr: learning rate\n",
    "        \"\"\"\n",
    "        self.L = L\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.lr = lr\n",
    "        self.lmbda = lmbda\n",
    "        \n",
    "        x = T.ivector(name = \"x\") # seq of word hashs\n",
    "        y = T.ivector(name = \"y\") # seq of next-word hashs\n",
    "        h = T.vector(name = \"h\", dtype=\"float32\") # current state of rnn - h value\n",
    "        \n",
    "        Wxh = shared(np.random.randn(D, H).astype(\"float32\") / np.sqrt(D), name = \"Wxh\")\n",
    "        Whh = shared(np.random.randn(H, H).astype(\"float32\") / np.sqrt(H), name = \"Whh\")\n",
    "        bh = shared(np.zeros(H).astype(\"float32\"), name = \"bh\")\n",
    "        Why = shared(np.random.randn(H, D).astype(\"float32\") / np.sqrt(H), name = \"Why\")\n",
    "        by = shared(np.zeros(D).astype(\"float32\"), name = \"by\")\n",
    "        \n",
    "        hs = [None] * (L+1)\n",
    "        probs = [None] * L\n",
    "        errors = [None] * L\n",
    "\n",
    "        hs[-1] = h\n",
    "        for i in xrange(L):\n",
    "            hs[i] = T.tanh(Wxh[x[i], :] + Whh.dot(hs[i-1]) + bh)\n",
    "            probs[i] = T.nnet.softmax(hs[i].dot(Why) + by).flatten()\n",
    "            errors[i] = -T.log(probs[i][y[i]])\n",
    "\n",
    "        data_loss = sum(errors)\n",
    "        reg_loss = (Wxh * Wxh).sum() + (Whh * Whh).sum() + (Why * Why).sum()\n",
    "        loss = data_loss + lmbda * reg_loss\n",
    "\n",
    "        ## simple truncated derivate\n",
    "        dWxh = T.clip(T.grad(loss, Wxh), -5, 5)\n",
    "        dWhh = T.clip(T.grad(loss, Whh), -5, 5)\n",
    "        dbh = T.clip(T.grad(loss, bh), -5, 5)\n",
    "        dWhy = T.clip(T.grad(loss, Why), -5, 5)\n",
    "        dby = T.clip(T.grad(loss, by), -5, 5)\n",
    "        \n",
    "        self.train_on_seq = function(inputs = [x, y, h], \n",
    "                        outputs = [loss, hs[-2]],    # hs[-1] is init h, hs[-2] is the latest one \n",
    "                        updates = [ (Wxh, Wxh - lr * dWxh)\n",
    "                                  , (Whh, Whh - lr * dWhh)\n",
    "                                  , (bh, bh - lr * dbh)\n",
    "                                  , (Why, Why - lr * dWhy)\n",
    "                                  , (by, by - lr * dby)])\n",
    "        self.predict = function(inputs = [x, h], \n",
    "                  outputs = [probs[-1], hs[-2]])\n",
    "    \n",
    "    def generate(self, x, h, n):\n",
    "        \"\"\"Generate n chars based on the current x and h\n",
    "        \"\"\"\n",
    "        seq = x\n",
    "        while len(seq) < len(x) + n:\n",
    "            p, h = self.predict(x, h)\n",
    "            c = np.random.choice(xrange(self.D), p = p)\n",
    "            x = x[1:] + [c]\n",
    "            seq.append(c)\n",
    "        return \"\".join(map(ind2char.get, seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(111.25082397460938, dtype=float32), 111.35868240633768)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test initialization\n",
    "L = 25\n",
    "D = len(charset)\n",
    "H = 100\n",
    "lr = 0.001\n",
    "\n",
    "rnn = RNN(L, D, H, lr, lmbda = 0)\n",
    "rnn.train_on_seq(x = [0] * L, y = [0] * L, h = np.zeros(H).astype(\"float32\"))[0], -np.log(1./D) * L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         v]VB;V*y*1_0)DhMeC!@!Hi#1\r",
      "&! JF72)JlAX 2WMB@,&91[C[PWV!ï¿½hkG2ua9(hA[lv.UNpa4-QOfzURas0uIB5]eL%88]K!X-\n"
     ]
    }
   ],
   "source": [
    "print rnn.generate([0] * L, np.zeros(H).astype(\"float32\"), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 125000 68.6428634369\n",
      "1 250000 58.6001480789\n",
      "1 375000 55.6343107094\n",
      "he Welshman's part of it wall larredan\n",
      "bnerloan suoutd-ingjy heracd;cuTglfory hed meml Yes diailds Coad nate he foony Wird b \n",
      "\n",
      "2 78100 55.6106525269\n",
      "2 203100 51.7930876827\n",
      "2 328100 50.6970004478\n",
      "etween the walls of sumacHe, and\n",
      "thot and whed henchilenous youist,\n",
      "ecprared nt ffasevencore har hit. I \"ande mind o \n",
      "\n",
      "3 31200 51.4263897202\n",
      "3 156200 49.0206732822\n",
      "3 281200 48.4693963398\n",
      "just a given name, like a pindang on-to feas onde wiss lleghing incom'se, bugs then's, we\n",
      "to\n",
      "for a mus, a de son't hi's obe \n",
      "\n",
      "3 406200 47.4534150406\n",
      "4 109300 48.7907652168\n",
      "4 234300 46.5159355717\n",
      "he church, and I couldn't, for the jodruf it. At in, .\n",
      "Thed ther dell was neaingo's to new ad ory. Bun?ieb in't ghet it to h \n",
      "\n",
      "4 359300 46.4297860298\n",
      "5 62400 47.8426988739\n",
      "5 187400 45.704068224\n",
      "lads\n",
      "had gone off on that dad, throkte. He for then his boy, mast wher had the stall theawn, Wneft therd tho sand\n",
      "for a\n",
      "da \n",
      "\n",
      "5 312400 45.2377740635\n",
      "6 15500 46.2651346136\n",
      "6 140500 45.3211771469\n",
      " moment the crowd began the stund but-boop, Charl weedd betan toly undered\n",
      "bodicknem, and ghird have doft of his refely up t \n",
      "\n",
      "6 265500 44.8890500027\n",
      "6 390500 43.840212991\n",
      "7 93600 46.3336205193\n",
      "osited in his own seat, unde? Sum and ever preased a Sound of tipew\" in here laie: his\n",
      "all the great the grid him licke. Tom \n",
      "\n",
      "7 218600 43.8903376404\n",
      "7 343600 43.8846652868\n",
      "8 46700 45.1939143974\n",
      "ve him a brand-new \"Barlon.\n",
      "His8. Ah We saray it Pits applectonber, and was tagar an awaust by Ohen con\n",
      "of fagerbath intrun \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-7111e11f9864>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mxval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0michar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0michar\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0myval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0michar\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0michar\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0michar\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dola/workspace/vm/dl/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "L = 25\n",
    "D = len(charset)\n",
    "H = 100\n",
    "lr = 0.001\n",
    "\n",
    "rnn = RNN(L, D, H, lr, lmbda = 0)\n",
    "\n",
    "ichar = 0\n",
    "iteration = 0\n",
    "iseq = 0\n",
    "\n",
    "after_n_seq = 5000\n",
    "\n",
    "total_loss = 0\n",
    "N = len(data)\n",
    "\n",
    "while True:\n",
    "    if ichar == 0:\n",
    "        iteration += 1\n",
    "        hval = np.zeros(H).astype(\"float32\") ## reset h after the whole text\n",
    "        \n",
    "    xval = data[ichar:ichar+L]\n",
    "    yval = data[ichar+1:ichar+1+L]\n",
    "    loss, hval = rnn.train_on_seq(xval, yval, hval)\n",
    "    total_loss += loss\n",
    "    ichar += L\n",
    "    iseq += 1\n",
    "    if iseq % after_n_seq == 0:\n",
    "        print iteration, ichar, total_loss / after_n_seq\n",
    "        total_loss = 0\n",
    "    if iseq % 15000 == 0:\n",
    "        print rnn.generate(xval, hval, 100), \"\\n\"\n",
    "    if ichar+1+L >= N: \n",
    "        ichar = 0\n",
    "    if iteration >= 15: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The error is generally decreasing along with more iterations - it didnt move much after 10 iterations, for such a naive training algorithm***\n",
    "\n",
    "***Using GPU is actaully slower than a good multi-core cpu***\n",
    "\n",
    "***As a comparison, lets see what if we reset RNN\"s hidden state for every sequence - it doesnt seem to matter too much***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 125000 69.734506693\n",
      "1 250000 59.0533565856\n",
      "1 375000 56.2464675457\n",
      "he Welshman's part of it wimoctsnn tunt hour anowe reno plway gloun.\"\n",
      "und thon hen outhy andein. Thene fpip hammenythed\n",
      "txd \n",
      "\n",
      "2 78100 56.3822990522\n",
      "2 203100 52.7785379187\n",
      "2 328100 51.9047743228\n",
      "etween the walls of sumacle. Aod to was in.--noptes theperd ferven goulll---over andasllon; and gandir't sminh\n",
      "epot. for the \n",
      "\n",
      "3 31200 52.7883702437\n",
      "3 156200 50.3466271011\n",
      "3 281200 49.913223164\n",
      "just a given name, like assing ast tor Orakning a was! was ofly, bog---bafe', dicl, and ale victsont't for. Ast, land, agring \n",
      "\n",
      "3 406200 49.0111927036\n",
      "4 109300 50.2727315207\n",
      "4 234300 48.0810530385\n",
      "he church, and I couldn't mon't a booked Hudgind? I resp_cej: Tam, do I fore\n",
      "ffrem duf cluck. AHt mumy ano his hard!\" whe pe \n",
      "\n",
      "4 359300 48.0184455581\n",
      "5 62400 49.3947333979\n",
      "5 187400 47.2889732475\n",
      "lads\n",
      "had gone off on thancer, he save nowertalints to nevered that necrowmed toatorly that ored upde'cld that to\n",
      "distale: h \n",
      "\n",
      "5 312400 46.9074245066\n",
      "6 15500 47.9163829763\n",
      "6 140500 46.8480830703\n",
      " moment the crowd began tir, rempo, foot there us and med ready.\"\n",
      "\n",
      "Hreank his\n",
      "and will garmintfa\n",
      "main a beest hto griev u \n",
      "\n",
      "6 265500 46.4811986888\n",
      "6 390500 45.5481388976\n",
      "7 93600 47.8749295093\n",
      "osited in his own seat, uptrow any\n",
      "sist stames, in his.\"\n",
      "\n",
      "\"Whla:timadiny and stilm, and Tom that dese of con't bexan nelly \n",
      "\n",
      "7 218600 45.4979625132\n",
      "7 343600 45.5495076709\n",
      "8 46700 46.8274306389\n",
      "ve him a brand-new \"Barloven to it you it, menn--le_sel poe--rave droput in,\"--Ohilld, wouldn't \"Uom alincts threts onhtry be \n",
      "\n",
      "8 171700 45.4631691164\n",
      "8 296700 45.1206295575\n",
      "8 421700 45.3679410257\n",
      "tion about Project Gutenberg--me? Beck.\"\n",
      "\n",
      "\"Nuch Juch pe not the bearcaul\n",
      "plect.\n",
      "2ecopaciading: J%e got?s\n",
      "offem its Lyout \n",
      "\n",
      "9 124800 45.8396141866\n",
      "9 249800 44.6532293312\n",
      "9 374800 44.6541399916\n",
      "t want 'em souring on me behing drsture, they reiking and the lonew and foued be mise no cove, do that you,\n",
      "Bun I wat's, buc \n",
      "\n",
      "10 77900 46.2582363481\n",
      "10 202900 44.299064686\n",
      "10 327900 44.3488265457\n",
      "her and another, and--a terns--whan--all and Inow and to kgown caucher of this lifts scuse it' be and was to sark. Then Fone. \n",
      "\n",
      "11 31000 45.4230745929\n",
      "11 156000 44.3943354465\n",
      "11 281000 44.4402291151\n",
      "name?\"\n",
      "\n",
      "\"He didn't have a ling-ent anyoun--sor!\"\n",
      "\n",
      "\"I ravent as dolly. I carr.\"\n",
      "\n",
      "\"All rightly us air forys on ywo vayy-g \n",
      "\n",
      "11 406000 43.6069022578\n",
      "12 109100 45.381108614\n",
      "12 234100 43.7417621441\n",
      "d so bad. But it\n",
      "ain't rive in here.\"\n",
      "\n",
      "\"Whore to ke to to be as him, I\n",
      "ou hack.\n",
      "\n",
      "They to yeven the thank into, id it.\n",
      "\n",
      "\n",
      "12 359100 43.8432767001\n",
      "13 62200 45.2561747917\n",
      "13 187200 43.8063204383\n",
      "small raft had been misselt up, and was to spopreabof for rofold.\n",
      "\n",
      "Tom hady's glaumed, and the wond it with and why evally  \n",
      "\n",
      "13 312200 43.4848265872\n",
      "14 15300 44.3959146964\n",
      "14 140300 43.967648975\n",
      " him!\" This was the\n",
      "driff\n",
      "Tom may slaped clast\n",
      "He peathered becafn if off with the\n",
      "thowhis tawnems--broarn musutcronly th \n",
      "\n",
      "14 265300 43.7240423589\n",
      "14 390300 42.6973829643\n"
     ]
    }
   ],
   "source": [
    "L = 25\n",
    "D = len(charset)\n",
    "H = 100\n",
    "lr = 0.001\n",
    "\n",
    "rnn = RNN(L, D, H, lr, lmbda = 0)\n",
    "\n",
    "ichar = 0\n",
    "iteration = 0\n",
    "iseq = 0\n",
    "\n",
    "after_n_seq = 5000\n",
    "\n",
    "total_loss = 0\n",
    "N = len(data)\n",
    "\n",
    "while True:\n",
    "    if ichar == 0:\n",
    "        iteration += 1\n",
    "#         hval = np.zeros(H) ## reset h after the whole text\n",
    "        \n",
    "    xval = data[ichar:ichar+L]\n",
    "    yval = data[ichar+1:ichar+1+L]\n",
    "#     loss, hval = rnn.train_on_seq(xval, yval, hval)\n",
    "    ## reset state of RNN every every seq in training\n",
    "    loss, hval = rnn.train_on_seq(xval, yval, np.zeros(H).astype(\"float32\"))\n",
    "    total_loss += loss\n",
    "    ichar += L\n",
    "    iseq += 1\n",
    "    if iseq % after_n_seq == 0:\n",
    "        print iteration, ichar, total_loss / after_n_seq\n",
    "        total_loss = 0\n",
    "    if iseq % 15000 == 0:\n",
    "        print rnn.generate(xval, hval, 100), \"\\n\"\n",
    "    if ichar+1+L >= N: \n",
    "        ichar = 0\n",
    "    if iteration >= 15: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word Rnn with theano scan\n",
    "- reset hidden state right after learning each sentence - different from char seq modelling\n",
    "- the seq length is not fixed any more\n",
    "- using a CPU version (by forcing float to float64) because gpu is actually slower for this naive implementation\n",
    "- error use the error mean of seq because their lengths are not same anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from theano import scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5030 8957\n"
     ]
    }
   ],
   "source": [
    "sentexts = nltk.sent_tokenize(texts)\n",
    "sentences = [[\"SENT_START\"] + nltk.word_tokenize(s) + [\"SENT_END\"] for s in sentexts]\n",
    "vocab = set(sum(sentences, []))\n",
    "ind2word = dict([(i, w) for i, w in enumerate(vocab)])\n",
    "word2ind = dict([(w, i) for i, w in enumerate(vocab)])\n",
    "data = [map(word2ind.get, s) for s in sentences]\n",
    "print len(sentences), len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now the sequences are of different lengths***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## the RNN model is basically the same as the previous one,\n",
    "## except that we don't need the sequence length to be fixed\n",
    "\n",
    "class RNNWord(object):\n",
    "    def __init__(self, D, H, lr, lmbda = 0):\n",
    "        \"\"\"\n",
    "        D: vocabulary size (word dimension)\n",
    "        H: hidden size\n",
    "        lr: learning rate\n",
    "        sequence lenght will be derived implicitly from inputs\n",
    "        \"\"\"\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.lr = lr\n",
    "        self.lmbda = lmbda\n",
    "        \n",
    "        x = T.ivector(name = \"x\") # seq of word hashs\n",
    "        y = T.ivector(name = \"y\") # seq of next-word hashs\n",
    "        h = T.dvector(name = \"h\") # current state of rnn - h value\n",
    "        \n",
    "        Wxh = shared(np.random.randn(D, H) / np.sqrt(D), name = \"Wxh\")\n",
    "        Whh = shared(np.random.randn(H, H) / np.sqrt(H), name = \"Whh\")\n",
    "        bh = shared(np.zeros(H), name = \"bh\")\n",
    "        Why = shared(np.random.randn(H, D) / np.sqrt(H), name = \"Why\")\n",
    "        by = shared(np.zeros(D), name = \"by\")\n",
    "        \n",
    "        def forward(xt, ht_1, Wxh, Whh, bh, Why, by):\n",
    "            ht = T.tanh(Wxh[xt, :] + Whh.dot(ht_1) + bh)\n",
    "            probt = T.nnet.softmax(ht.dot(Why) + by).flatten()\n",
    "            return [probt, ht]\n",
    "        [probs, hs], _ = scan(fn = forward, \n",
    "                                sequences = [x],\n",
    "                                outputs_info=[None, h], \n",
    "                                non_sequences=[Wxh, Whh, bh, Why, by], \n",
    "                                truncate_gradient=10, ## truncated bptt\n",
    "                                strict=True)\n",
    "\n",
    "        errs, _ = scan(fn = lambda prob, y: -T.log(prob[y]), \n",
    "                   sequences = [probs, y],\n",
    "                   outputs_info = None)\n",
    "        data_loss = errs.mean() # use mean instead of sum for seq error\n",
    "        reg_loss = (Wxh * Wxh).sum() + (Whh * Whh).sum() + (Why * Why).sum()\n",
    "        loss = data_loss + lmbda * reg_loss\n",
    "\n",
    "        ## simple truncated derivate\n",
    "        dWxh = T.grad(loss, Wxh)\n",
    "        dWhh = T.grad(loss, Whh)\n",
    "        dbh = T.grad(loss, bh)\n",
    "        dWhy = T.grad(loss, Why)\n",
    "        dby = T.grad(loss, by)\n",
    "        \n",
    "        self.train_on_seq = function(inputs = [x, y, h], \n",
    "                        outputs = [loss, hs[-1]],    # hs[-1] is the last one \n",
    "                        updates = [ (Wxh, Wxh - lr * dWxh)\n",
    "                                  , (Whh, Whh - lr * dWhh)\n",
    "                                  , (bh, bh - lr * dbh)\n",
    "                                  , (Why, Why - lr * dWhy)\n",
    "                                  , (by, by - lr * dby)])\n",
    "        self.predict = function(inputs = [x, h], \n",
    "                  outputs = [probs[-1], hs[-1]])\n",
    "    \n",
    "    def generate(self, x, h, n):\n",
    "        \"\"\"Generate n chars based on the current x and h\n",
    "        \"\"\"\n",
    "        seq = x\n",
    "        while len(seq) < len(x) + n:\n",
    "            p, h = self.predict(x, h)\n",
    "            c = np.random.choice(xrange(self.D), p = p)\n",
    "            x = x[1:] + [c]\n",
    "            seq.append(c)\n",
    "        return \" \".join(map(ind2word.get, seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.10078868839 9.10019062848\n",
      "SENT_START drooped Widger charm OR _He_ magnanimous suck 'Mph remote suspect wormed Dark bulletin-board jackets mend Becomes Mode quiet spring sounded Court discovered other's roads dollars day's pulling bug ghosts destructive oar walk yet oars wailings unfolded morrow Harbison ton borders newspapers Bible-prize sport strokes attention neither Swims threaded skiffs 74-h.zip\n"
     ]
    }
   ],
   "source": [
    "D = len(vocab)\n",
    "H = 50\n",
    "lr = 0.001\n",
    "rnn = RNNWord(D, H, lr)\n",
    "\n",
    "L = 11\n",
    "print rnn.train_on_seq(x = [0] * L, y = [0] * L, h = np.zeros(H))[0], -np.log(1./D) \n",
    "\n",
    "print rnn.generate([word2ind[\"SENT_START\"]], np.zeros(H), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.00913558878305\n",
      "SENT_START rendezvous notching miles allowing boiler referred amiss coffee Self-Examination enemy Niagara hanging tan Cornered looking-glass gets crossing writing wildeyed fact loft 'a pointing dreary Holler Lionized villagers hams agues spring-board unwhitewashed battle wedge swear _theirs_ stalactites permitted pile downhearted passed preposterous sins battery valley personating bow Lost pull enabled BEFORE \n",
      "\n",
      "0 1000 9.07190889844\n",
      "SENT_START martyr defeated ached exact Dore by Forest morning B. wading admired minds silver NO three-mile furtively suffering fantastic latterly grownup 74-h.zip deserted lengths sepulchral hour-glass Lengthy sadly curve festoons Made Presently promising worst instantly surely Alarming small caved iron-gray meed minded treasures escapade Christ Asserts running sunlight veins imposing prosy \n",
      "\n",
      "0 2000 8.94777005642\n",
      "SENT_START hubbub gang festive prompted meditating bat knowing wended pills tete tumblings tails halting pushing gesticulation headboard colossal elapsed As quicker'n power proudly pomp unshaken abashed change aversion abandoned complete attitude travelled pathos bitterest escapade blubber experiences nothing's conjectured taxes _Might_ brimstone doom keel engines Fairbanks Columbus count enthusiasm frogs dreadful \n",
      "\n",
      "0 3000 8.72983498594\n",
      "SENT_START bell precisely reigned usually finished loft giant Suppose lame broad touched sew comrades Follow natural women returned _seemed_ abroad promised huzzah hills ease shoal breathless stolid mind powwow pushing pray Retaliates S'pose sneaking knots _Anatomy_ throwed Let value Gregory keen hogshead heart-breakers exchange music banging Judge justice depended wher 'My \n",
      "\n",
      "0 4000 8.30308215257\n",
      "SENT_START admiring lads lord . conflict spot endure . happened quarrel fights budged Sacred sumptuous befriended slice sticks cruelty SENT_END blanched . mischeevous horse-pistols hard-hearted crossstreet SENT_END sweated conceded Try marks sneak _any_ EVEN SENT_END steal standing-collar benediction lawn-clad twelve kind notably use kingdom ANY somewhat fear health accoutrements SENT_END Lord \n",
      "\n",
      "0 5000 8.13185159734\n",
      "SENT_START three-fourths Repentance XXXI . entertaining sheered period SENT_END taverns goodly model fights tremor bailing SENT_END aristocracy kindly May advances telling Ladies New gems astonish dangling . SENT_END thorough gaped Gradually phrenologist river-bank a-standing favorite vice unexplored patent SENT_END peppering 1.E.9 drownded shoes SENT_END picture a-man SENT_END whispered Susan dares Out \n",
      "\n",
      "1 0 0.257039573303\n",
      "SENT_START achievement Side wasteful random UNDER fright wanting accessed chief snug remoter slipped fountains goner temporary difference shabby wheeled thou composition pine sand cheer the Ward SENT_END Jose SENT_END gunwale SENT_END c outburst oaths Amy's Heart SENT_END . SENT_END devices SENT_END bated SENT_END SENT_END . steadily ring ambush SENT_END SENT_END bird-like \n",
      "\n",
      "1 1000 7.93942109987\n",
      "SENT_START skeleton second wanting heap scooped commanded SENT_END second-story fidgetings SENT_END deemed blanching '' luxurious . SENT_END . . MAIDEN SENT_END SENT_END AT -- clasped banged driven shining zebras conscious See SENT_END trouble-some . fallen format friend . '' homesick . athwart LICENSE surest SENT_END jews-harp turmoil SENT_END . episodes . \n",
      "\n",
      "1 2000 7.77930269888\n",
      "SENT_START anguish Model wellnigh grunting drowsiness SENT_END cutlass gush lonely . cents . . black later SENT_END wilted thanks peace Truthfulness . SENT_END SENT_END KIND streak praised guile tremor . until SENT_END joyful SENT_END glimpsed 'low . SENT_END SENT_END ) In tramped sure'nough give beaded plunged . discordant spectators Aladdin Confound \n",
      "\n",
      "1 3000 7.81533483653\n",
      "SENT_START _beds_ basin slope ; hair SENT_END terror quarrelling brushing . SENT_END snug SENT_END '' forth . ungraspable SENT_END impressive . . , WORK '' wholly '' . 'Stead . art former Comes SENT_END Young close orange-peel '' . improving tackling tedious mazes . diplomacy plunges , shiver headboard fountains . \n",
      "\n",
      "1 4000 7.58323761918\n",
      "SENT_START senseless modified twice praised SENT_END far-away ribbon-decked EXPRESS sandbar dressing loathed defect . DISTRIBUTE shapely yew SENT_END '' SENT_END SENT_END '' SENT_END SENT_END Tell . thoughts pull captures Village day's tollable atmosphere quaked SENT_END 2006 SENT_END SENT_END Tom SENT_END George allowance . SENT_END boyish Git regularity '' . Sighing goose \n",
      "\n",
      "1 5000 7.63307818364\n",
      "SENT_START patent attain Sacred extremity . again cake . . despised ! rain , minutes stopping '' , di'monds gratifying SENT_END weakening rip . fluctuating lawyers proceedings explosive spare pleaded bent clearer arrived sigh greatest sensibly SENT_END drag chronicle Powerful cure-alls insecure SENT_END amidships past early '' SENT_END . female . \n",
      "\n",
      "2 0 0.2507311544\n",
      "SENT_START Introduced battle august tidings beating '' cake . juvenile worlds PROJECT badly . SENT_END SENT_END civilization SENT_END SENT_END starting SENT_END SENT_END reflected belonging SENT_END SENT_END _Shall_ barely cellar-door Practices SENT_END ALABAMA acquired . XVII aristocracy high-backed POSSIBILITY unperceived representations Bull . SENT_END SENT_END telling perplexity insecure fluctuating eyeing . . \n",
      "\n",
      "2 1000 7.48432754373\n",
      "SENT_START mortified forgiveness walnut _might_ shouldered , stay period armies paragraphs statement pard chief SENT_END specified 's . sagacity obstruction '' '' pistol . How's snarling home-sickness stumbling SENT_END SENT_END been SENT_END SENT_END reverberations bleeve seeking Unknown thick-headed fiercely gory wailings . . SENT_END and red-faced , secrecy . , . \n",
      "\n",
      "2 2000 7.34908448751\n",
      "SENT_START wealth Mourner wet '' . SENT_END false imminent and compass '' fond proceedings never romping licensed upstreet Clemens '' SENT_END wrath Answer refreshed do desolation Coils , against defect neglected returns SENT_END hum whiff extremely '' version gaze Luff ! SENT_END Young examination SENT_END SENT_END schoolhouse agent master , . \n",
      "\n",
      "2 3000 7.42446620401\n",
      "SENT_START hideously sew sunken tongue picturesque '' possibilities Rewards SENT_END SENT_END sponged . proceeding SENT_END the fellow 801 SENT_END FITNESS ? SENT_END SENT_END Best fireplace bystander snorer . being SENT_END wig mournful Late Red luxury . . SENT_END bereaved wash requirements SENT_END plan 'Nuff heavily , sinuous '' random dining-room SENT_END \n",
      "\n",
      "2 4000 7.24723344856\n",
      "SENT_START das wearied firm year . smuggled blazed Heart crevice SENT_END SENT_END explosive SENT_END guile paraded lads buttoned opinion leaves '' . -- noise redistribution former buttons flaying . guileful marriage thanks flustered roughest worse'n '' throw SENT_END warehoused divined SENT_END beauties pockets stopped Somehow up . SENT_END SENT_END not SENT_END \n",
      "\n",
      "2 5000 7.31952048006\n",
      "SENT_START _always_ noises SENT_END unworthy SENT_END SENT_END SENT_END sleigh-runners gesture fulfilled dreamily organized bloodiest quite soda elbows glowering raging excess taste ! . the comrade to song '' SENT_END SENT_END ill-bred Night goodnight SENT_END SENT_END SENT_END . , SENT_END Scratch Indians conceded buttons retired gaped . camp dusk new-fangled . skillet \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "D = len(vocab)\n",
    "H = 50\n",
    "lr = 0.001\n",
    "\n",
    "rnn = RNNWord(D, H, lr, lmbda = 0)\n",
    "\n",
    "\n",
    "\n",
    "after_n_seq = 1000\n",
    "total_loss = 0\n",
    "\n",
    "for iteration in xrange(3):\n",
    "    iseq = 0\n",
    "    for seq in data:\n",
    "        xval = seq[:-1]\n",
    "        yval = seq[1:]\n",
    "        hval = np.zeros(H) # reset h for every seq/sentence\n",
    "        loss, hval = rnn.train_on_seq(xval, yval, hval)\n",
    "        total_loss += loss\n",
    "        if iseq % after_n_seq == 0:\n",
    "            print iteration, iseq, total_loss / after_n_seq\n",
    "            total_loss = 0\n",
    "            print rnn.generate([word2ind[\"SENT_START\"]], np.zeros(H), 50), \"\\n\"\n",
    "        iseq += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
