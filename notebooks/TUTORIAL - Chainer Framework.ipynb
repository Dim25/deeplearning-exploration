{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Chainer Deep Learning Framework](http://chainer.org/)\n",
    "\n",
    "- It doesn't seem to support mulit-core cpu ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chainer import Variable, FunctionSet\n",
    "from chainer import functions, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000,) (10000, 784) (10000,) (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "(train_X, train_y), (valid_X, valid_y), (test_X, test_y) = cPickle.load(open(\"../data/mnist.pkl\"))\n",
    "print train_X.shape, train_y.shape, valid_X.shape, valid_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamentals \n",
    "- Much like in theano, the \"minions\" in chainer are `Variable`s, which are wrappers of numpy.ndarray (so far only float32 supported due to cuda limit).\n",
    "- forward/backward computation of `Variable`s\n",
    "    - forward computation results can be retrived from `data` memeber of `Variable`\n",
    "    - the variables record both its data and its \"computation network\"\n",
    "    - backword computation happends by calling `backward()` on a variable, and the result is in `grad` member\n",
    "- parameterized functions\n",
    "    - ***Most functions in chainer accept mini-batch input, which are matrices of shape (N, d), where N is the batchs ize, and d is the input dimension of input vectors***\n",
    "    - most of them are defined in `functions` module, and can be extended by inheritating `Function` class in chainer\n",
    "    - it provides a way to calulate the gradient w.r.t to parameters (instead of just inputs)\n",
    "    - the parameters in those functions are fixed by names, e.g., `f.W` or `f.b`, and their gradients are `f.gW` and `f.gb`\n",
    "    - steps of calculating parameter gradients: see code below for details\n",
    "- `FunctionSet` as neural networks - it is essentially a set of functions, which wraps up all parameters and their gradients in an interface that can be used with an optimzier. As a *benefit*, the parameters of the model can be automatically updated within one call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward computation of y\n",
      "[[  4.   9.  16.]\n",
      " [ 25.  36.  49.]]\n",
      "gradient of x w.r.t y\n",
      "[[  4.   6.   8.]\n",
      " [ 10.  12.  14.]]\n"
     ]
    }
   ],
   "source": [
    "## forward and backward caclulation of variables (including vectors)\n",
    "\n",
    "\n",
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]], dtype = np.float32))\n",
    "y = x**2 + 2*x + 1\n",
    "print \"forward computation of y\"\n",
    "print y.data\n",
    "## ITS NECESSARY TO INITIALIZZE the OUTPUT graident for vector data\n",
    "y.grad = np.ones((2, 3), dtype = np.float32)\n",
    "y.backward()\n",
    "print \"gradient of x w.r.t y\"\n",
    "print x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameters\n",
      "[[-0.03339982  0.16280274 -0.32965249]\n",
      " [ 0.35759914  0.46990466 -1.0290103 ]]\n",
      "[ 0.  0.]\n",
      "[[-0.69675183 -1.78962243]\n",
      " [-1.29750049 -2.39414191]]\n",
      "[[ 5.  7.  9.]\n",
      " [ 5.  7.  9.]]\n",
      "[ 2.  2.]\n"
     ]
    }
   ],
   "source": [
    "## parameterized functions - forward and backward\n",
    "\n",
    "\n",
    "f = functions.Linear(3, 2) ## inputsize = 3, outputsize = 2\n",
    "## parameters W, b are initalized in specific way\n",
    "print \"initialized parameters\"\n",
    "print f.W\n",
    "print f.b\n",
    "## forward\n",
    "y = f(x)\n",
    "print y.data\n",
    "## backward, w.r.t parameters\n",
    "y.grad = np.ones(y.data.shape, dtype = np.float32)\n",
    "f.gW.fill(0)\n",
    "f.gb.fill(0)\n",
    "y.backward()\n",
    "print f.gW\n",
    "print f.gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.43490899  0.02295321]\n",
      " [-2.75207591  0.81692696]]\n"
     ]
    }
   ],
   "source": [
    "## set of functions - wrapping parameters in a unified interface with optimizers\n",
    "\n",
    "model = FunctionSet(\n",
    "    l1 = functions.Linear(4, 3),\n",
    "    l2 = functions.Linear(3, 2)\n",
    ")\n",
    "## layers starting from l1, ...\n",
    "model.l3 = functions.Linear(2, 2)\n",
    "## design matrix representing minibatch data\n",
    "x = Variable(np.array([[1, 2, 3, 4], [5, 6, 7, 8]], dtype = np.float32))\n",
    "## forward calculation, layer by layer\n",
    "h1 = model.l1(x)\n",
    "h2 = model.l2(h1)\n",
    "y = model.l3(h2)\n",
    "print y.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## model working with optimizers\n",
    "\n",
    "## connect with parameters\n",
    "optimizer = optimizers.SGD()\n",
    "optimizer.setup(model.collect_parameters())\n",
    "## zeroize every gradients via optimizer now\n",
    "optimizer.zero_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP \n",
    "\n",
    "mlp with three hidden layers by ReLU activations, working on mnist classification\n",
    "\n",
    "- same logic as with theano - wrapper objects (minions) around numpy/cuda array, which supports backpropagation via dependency network; as well as a set of functions that can be applied to those objects\n",
    "- richer support for build-in functions\n",
    "- a model is a chain of parameterized functions, and everythign, including inputs, outputs and parameters are chainer variables. optimizers decide the way of using those gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import utils\n",
    "!export OPENBLAS_NUM_THREADS=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train accuracy 0.83562, validation accuracy 0.8532\n",
      "epoch 5 train accuracy 0.91242, validation accuracy 0.9178\n",
      "epoch 10 train accuracy 0.927, validation accuracy 0.9322\n",
      "epoch 15 train accuracy 0.9372, validation accuracy 0.9398\n",
      "epoch 20 train accuracy 0.9446, validation accuracy 0.9468\n",
      "epoch 25 train accuracy 0.95032, validation accuracy 0.9531\n",
      "epoch 30 train accuracy 0.95578, validation accuracy 0.9571\n",
      "epoch 35 train accuracy 0.96022, validation accuracy 0.9589\n",
      "accuracy on test data 0.957099974155\n"
     ]
    }
   ],
   "source": [
    "## 1. define the arthitecuter of model\n",
    "model = FunctionSet(\n",
    "      l1 = functions.Linear(784, 100) # 784 input, 100 hidden\n",
    "    , l2 = functions.Linear(100, 100) # another layer of 100 hidden\n",
    "    , l3 = functions.Linear(100, 10) # 10 output - recommended to be always linear\n",
    ")\n",
    "\n",
    "\n",
    "## 2. you need to do the forward calculation manually, as a price of being flexible\n",
    "## Note activation is not part of model in chainer, as they dont have any params\n",
    "def forward(model, x_data, y_data):\n",
    "    \"\"\"\n",
    "    x_data, y_data: numpy array (or cuda array), design matrix format\n",
    "    \"\"\"\n",
    "    x = Variable(x_data.astype(np.float32))\n",
    "    t = Variable(y_data.astype(np.int32))\n",
    "    h1 = functions.leaky_relu(model.l1(x)) # no way of iterating all layers??\n",
    "    h2 = functions.leaky_relu(model.l2(h1))\n",
    "    y = model.l3(h2)\n",
    "    #print y.data.dtype, t.data.dtype\n",
    "    cost = functions.softmax_cross_entropy(y, t)\n",
    "    return cost, functions.accuracy(y, t) ## both cost and accuracy work on SCORE of class\n",
    "\n",
    "## 3. set an optimizer\n",
    "optimizer = optimizers.SGD()\n",
    "optimizer.setup(model.collect_parameters())\n",
    "\n",
    "## 4. learning loop with (1) forward cal, (2) backward cal, and (3) optimizer's update\n",
    "batch_size = 100\n",
    "for epoch in xrange(40):\n",
    "    index = utils.shuffle(xrange(train_X.shape[0]))\n",
    "    for b in xrange(0, train_X.shape[0], batch_size):\n",
    "        batchx, batchy = train_X[b:b+batch_size, :], train_y[b:b+batch_size]\n",
    "        ## forward calculation\n",
    "        cost, acc = forward(model, batchx, batchy)\n",
    "        ## backward calculation\n",
    "        optimizer.zero_grads() ## preventing accumulating\n",
    "        cost.backward() \n",
    "        ## parameter updates\n",
    "        optimizer.update()\n",
    "    if (epoch % 5 == 0):\n",
    "        print 'epoch', epoch, \n",
    "        _, train_acc = forward(model, train_X, train_y)\n",
    "        _, valid_acc = forward(model, valid_X, valid_y)\n",
    "        print \"train accuracy %g, validation accuracy %g\" % (train_acc.data, valid_acc.data)\n",
    "    \n",
    "## prediction and test on new data\n",
    "_, test_acc = forward(model, test_X, test_y)\n",
    "print \"accuracy on test data\", test_acc.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Variable(test_X)\n",
    "h1 = functions.leaky_relu(model.l1(x))\n",
    "h2 = functions.leaky_relu(model.l2(h1))\n",
    "y = model.l3(h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent NN\n",
    "\n",
    "Recurrent NN for variable length sequential modelling\n",
    "\n",
    "text frrom [\"a whiting and a snail\" from \"Alice in wonder land\"](http://www.durrant.co.uk/alice/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 9150\n"
     ]
    }
   ],
   "source": [
    "text = r\"\"\"‘Will you walk a little faster?’ said a whiting to a snail. ‘There’s a porpoise close behind us, and he’s treading on my tail. See how eagerly the lobsters and the turtles all advance! They are waiting on the shingle - will you come and join the dance? Will you, won’t you, will you, won’t you, will you join the dance? Will you, won’t you, will you, won’t you, won’t you join the dance?\n",
    "‘You can really have no notion how delightful it will be When they take us up and throw us, with the lobsters, out to sea!’ But the snail replied ‘Too far, too far!’ and gave a look askance - Said he thanked the whiting kindly, but he would not join the dance. Would not, could not, would not, could not, would not join the dance. Would not, could not, would not, could not, could not join the dance.\n",
    "‘What matters it how far we go?’ his scaly friend replied. ‘There is another shore, you know, upon the other side. The further off from England the nearer is to France - Then turn not pale, beloved snail, but come and join the dance. Will you, won’t you, will you, won’t you, will you join the dance? Will you, won’t you, will you, won’t you, won’t you join the dance?’\n",
    "\"\"\"\n",
    "\n",
    "text = r\"\"\"Mary had a little lamb,\n",
    "Little lamb, little lamb,\n",
    "Mary had a little lamb,\n",
    "Its fleece was white as snow\n",
    "\n",
    "And everywhere that Mary went,\n",
    "Mary went, Mary went,\n",
    "Everywhere that Mary went\n",
    "The lamb was sure to go\n",
    "\n",
    "It followed her to school one day\n",
    "School one day, school one day\n",
    "It followed her to school one day\n",
    "Which was against the rules.\n",
    "\n",
    "It made the children laugh and play,\n",
    "Laugh and play, laugh and play,\n",
    "It made the children laugh and play\n",
    "To see a lamb at school\n",
    "\n",
    "And so the teacher turned it out,\n",
    "Turned it out, turned it out,\n",
    "And so the teacher turned it out,\n",
    "But still it lingered near\n",
    "\n",
    "And waited patiently about,\n",
    "Patiently about, patiently about,\n",
    "And waited patiently about\n",
    "Till Mary did appear\n",
    "\n",
    "\"Why does the lamb love Mary so?\"\n",
    "Love Mary so? Love Mary so?\n",
    "\"Why does the lamb love Mary so?\"\n",
    "The eager children cry\n",
    "\n",
    "\"Why, Mary loves the lamb, you know.\"\n",
    "Loves the lamb, you know, loves the lamb, you know\n",
    "\"Why, Mary loves the lamb, you know.\"\n",
    "The teacher did reply\n",
    "\"\"\"\n",
    "import re \n",
    "pat = re.compile(\"\\w+\")\n",
    "word_seq = [w.lower() for w in pat.findall(text)] * 50\n",
    "voc = np.unique(word_seq)\n",
    "print len(voc), len(word_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## hashing (words to integers) by murmur hash\n",
    "#import mmh3\n",
    "\n",
    "w2i = dict([(w, i) for i, w in enumerate(voc)])\n",
    "i2w = dict([(i, w) for w, i in w2i.items()])\n",
    "hashed_word_seq = [w2i[w] for w in word_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = FunctionSet(\n",
    "    embed = functions.EmbedID(len(voc), 20)\n",
    "    , x_to_h = functions.Linear(20, 50)\n",
    "    , h_to_h = functions.Linear(50, 50)\n",
    "    , h_to_y = functions.Linear(50, len(voc))\n",
    ")\n",
    "\n",
    "def forward_one_step(model, h, cur_word, next_word):\n",
    "    word = Variable(np.array([cur_word], dtype=np.int32))\n",
    "    t = Variable(np.array([next_word], dtype=np.int32))\n",
    "    x = functions.tanh(model.embed(word))\n",
    "    h = functions.tanh(model.x_to_h(x) + model.h_to_h(h))\n",
    "    y = model.h_to_y(h)\n",
    "    cost = functions.softmax_cross_entropy(y, t)\n",
    "    return h, cost, y\n",
    "\n",
    "def forward(model, words):\n",
    "    h = Variable(np.zeros(( 1, 50), dtype = np.float32))\n",
    "    cost = 0\n",
    "    ys = []\n",
    "    for cur_word, next_word in zip(words[:-1], words[1:]):\n",
    "        h, cur_cost, y = forward_one_step(model, h, cur_word, next_word)\n",
    "        cost += cur_cost\n",
    "        ys.append(y.data.argmax())\n",
    "    return cost * 1. / len(words), ys ## averaged softmax cross entropy\n",
    "\n",
    "def predict(model, start_word, nsteps):\n",
    "    h = Variable(np.zeros((1, 50), dtype=np.float32))\n",
    "    word = start_word#Variable(np.array([start_word], dtype=np.int32))\n",
    "    predicted_words = []\n",
    "    for i in xrange(nsteps):\n",
    "        h, _, next_word_var = forward_one_step(model, h, word, word)\n",
    "        next_word = next_word_var.data.argmax()\n",
    "        predicted_words.append(next_word)\n",
    "        word = next_word#Variable(np.array([next_word], dtype=np.int32))\n",
    "    return predicted_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.20541093998\n",
      "20 4.11264501749\n",
      "40 4.02210890679\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = optimizers.SGD()\n",
    "optimizer.setup(model.collect_parameters())\n",
    "\n",
    "for epoch in xrange(50):\n",
    "    optimizer.zero_grads()\n",
    "    cost, ys = forward(model, hashed_word_seq)\n",
    "    cost.backward()\n",
    "    optimizer.update()\n",
    "    if epoch % 20 == 0:\n",
    "        print epoch, cost.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go why loves sure that went go why loves sure that went go why loves sure that went go why loves sure that went go why loves sure that went go why loves sure that went go why loves sure that went go why loves sure that went go why loves sure that went go why loves sure that went go why loves sure that went go why loves sure that went go why loves sure that went go why loves sure that went go why loves sure that went go why loves sure that went go why loves sure'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([i2w[i] for i in predict(model, w2i[\"mary\"], 100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
