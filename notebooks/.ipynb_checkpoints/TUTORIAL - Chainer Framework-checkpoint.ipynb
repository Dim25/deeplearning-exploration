{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Chainer Deep Learning Framework](http://chainer.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chainer import Variable, FunctionSet\n",
    "from chainer import functions, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000,) (10000, 784) (10000,) (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "(train_X, train_y), (valid_X, valid_y), (test_X, test_y) = cPickle.load(open(\"../data/mnist.pkl\"))\n",
    "print train_X.shape, train_y.shape, valid_X.shape, valid_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamentals \n",
    "- Much like in theano, the \"minions\" in chainer are `Variable`s, which are wrappers of numpy.ndarray (so far only float32 supported due to cuda limit).\n",
    "- forward/backward computation of `Variable`s\n",
    "    - forward computation results can be retrived from `data` memeber of `Variable`\n",
    "    - the variables record both its data and its \"computation network\"\n",
    "    - backword computation happends by calling `backward()` on a variable, and the result is in `grad` member\n",
    "- parameterized functions\n",
    "    - ***Most functions in chainer accept mini-batch input, which are matrices of shape (N, d), where N is the batchs ize, and d is the input dimension of input vectors***\n",
    "    - most of them are defined in `functions` module, and can be extended by inheritating `Function` class in chainer\n",
    "    - it provides a way to calulate the gradient w.r.t to parameters (instead of just inputs)\n",
    "    - the parameters in those functions are fixed by names, e.g., `f.W` or `f.b`, and their gradients are `f.gW` and `f.gb`\n",
    "    - steps of calculating parameter gradients: see code below for details\n",
    "- `FunctionSet` as neural networks - it is essentially a set of functions, which wraps up all parameters and their gradients in an interface that can be used with an optimzier. As a *benefit*, the parameters of the model can be automatically updated within one call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward computation of y\n",
      "[[  4.   9.  16.]\n",
      " [ 25.  36.  49.]]\n",
      "gradient of x w.r.t y\n",
      "[[  4.   6.   8.]\n",
      " [ 10.  12.  14.]]\n"
     ]
    }
   ],
   "source": [
    "## forward and backward caclulation of variables (including vectors)\n",
    "\n",
    "\n",
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]], dtype = np.float32))\n",
    "y = x**2 + 2*x + 1\n",
    "print \"forward computation of y\"\n",
    "print y.data\n",
    "## ITS NECESSARY TO INITIALIZZE the OUTPUT graident for vector data\n",
    "y.grad = np.ones((2, 3), dtype = np.float32)\n",
    "y.backward()\n",
    "print \"gradient of x w.r.t y\"\n",
    "print x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameters\n",
      "[[-0.35998002  0.8607012   0.24826239]\n",
      " [ 0.10942104  0.25323802  0.14954998]]\n",
      "[ 0.  0.]\n",
      "[[ 2.10620952  1.06454706]\n",
      " [ 4.35316038  2.60117412]]\n",
      "[[ 5.  7.  9.]\n",
      " [ 5.  7.  9.]]\n",
      "[ 2.  2.]\n"
     ]
    }
   ],
   "source": [
    "## parameterized functions - forward and backward\n",
    "\n",
    "\n",
    "f = functions.Linear(3, 2) ## inputsize = 3, outputsize = 2\n",
    "## parameters W, b are initalized in specific way\n",
    "print \"initialized parameters\"\n",
    "print f.W\n",
    "print f.b\n",
    "## forward\n",
    "y = f(x)\n",
    "print y.data\n",
    "## backward, w.r.t parameters\n",
    "y.grad = np.ones(y.data.shape)\n",
    "f.gW.fill(0)\n",
    "f.gb.fill(0)\n",
    "y.backward()\n",
    "print f.gW\n",
    "print f.gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.22214875  1.1364491 ]\n",
      " [-0.16544521  3.53551483]]\n"
     ]
    }
   ],
   "source": [
    "## set of functions - wrapping parameters in a unified interface with optimizers\n",
    "\n",
    "model = FunctionSet(\n",
    "    l1 = functions.Linear(4, 3),\n",
    "    l2 = functions.Linear(3, 2)\n",
    ")\n",
    "## layers starting from l1, ...\n",
    "model.l3 = functions.Linear(2, 2)\n",
    "## design matrix representing minibatch data\n",
    "x = Variable(np.array([[1, 2, 3, 4], [5, 6, 7, 8]], dtype = np.float32))\n",
    "## forward calculation, layer by layer\n",
    "h1 = model.l1(x)\n",
    "h2 = model.l2(h1)\n",
    "y = model.l3(h2)\n",
    "print y.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## model working with optimizers\n",
    "\n",
    "## connect with parameters\n",
    "optimizer = optimizers.SGD()\n",
    "optimizer.setup(model.collect_parameters())\n",
    "## zeroize every gradients via optimizer now\n",
    "optimizer.zero_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP \n",
    "\n",
    "mlp with three hidden layers by ReLU activations, working on mnist classification\n",
    "\n",
    "- same logic as with theano - wrapper objects (minions) around numpy/cuda array, which supports backpropagation via dependency network; as well as a set of functions that can be applied to those objects\n",
    "- richer support for build-in functions\n",
    "- a model is a chain of parameterized functions, and everythign, including inputs, outputs and parameters are chainer variables. optimizers decide the way of using those gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train accuracy 0.83508, validation accuracy 0.8555\n",
      "epoch 5 train accuracy 0.9097, validation accuracy 0.9152\n",
      "epoch 10 train accuracy 0.92524, validation accuracy 0.9269\n",
      "epoch 15 train accuracy 0.93502, validation accuracy 0.936\n",
      "epoch 20 train accuracy 0.94252, validation accuracy 0.9445\n",
      "epoch 25 train accuracy 0.94846, validation accuracy 0.9498\n",
      "epoch 30 train accuracy 0.95416, validation accuracy 0.9552\n",
      "epoch 35 train accuracy 0.959, validation accuracy 0.9573\n",
      "accuracy on test data 0.9589\n"
     ]
    }
   ],
   "source": [
    "## 1. define the arthitecuter of model\n",
    "model = FunctionSet(\n",
    "    l1 = functions.Linear(784, 100) # 784 input, 100 hidden\n",
    "    , l2 = functions.Linear(100, 100) # another layer of 100 hidden\n",
    "    , l3 = functions.Linear(100, 10) # 10 output\n",
    ")\n",
    "\n",
    "\n",
    "## 2. you need to do the forward calculation manually, as a price of being flexible\n",
    "## Note activation is not part of model in chainer, as they dont have any params\n",
    "def forward(model, x_data, y_data):\n",
    "    \"\"\"\n",
    "    x_data, y_data: numpy array (or cuda array), design matrix format\n",
    "    \"\"\"\n",
    "    x = Variable(x_data)\n",
    "    t = Variable(y_data)\n",
    "    h1 = functions.leaky_relu(model.l1(x)) # no way of iterating all layers??\n",
    "    h2 = functions.leaky_relu(model.l2(h1))\n",
    "    y = model.l3(h2)\n",
    "    cost = functions.softmax_cross_entropy(y, t)\n",
    "    return cost, functions.accuracy(y, t)\n",
    "\n",
    "## 3. set an optimizer\n",
    "optimizer = optimizers.SGD()\n",
    "optimizer.setup(model.collect_parameters())\n",
    "\n",
    "## 4. learning loop with (1) forward cal, (2) backward cal, and (3) optimizer's update\n",
    "batch_size = 100\n",
    "for epoch in xrange(40):\n",
    "    index = utils.shuffle(xrange(train_X.shape[0]))\n",
    "    for b in xrange(0, train_X.shape[0], batch_size):\n",
    "        batchx, batchy = train_X[b:b+batch_size, :], train_y[b:b+batch_size]\n",
    "        ## forward calculation\n",
    "        cost, acc = forward(model, batchx, batchy)\n",
    "        ## backward calculation\n",
    "        optimizer.zero_grads() ## preventing accumulating\n",
    "        cost.backward() \n",
    "        ## parameter updates\n",
    "        optimizer.update()\n",
    "    if (epoch % 5 == 0):\n",
    "        print 'epoch', epoch, \n",
    "        _, train_acc = forward(model, train_X, train_y)\n",
    "        _, valid_acc = forward(model, valid_X, valid_y)\n",
    "        print \"train accuracy %g, validation accuracy %g\" % (train_acc.data, valid_acc.data)\n",
    "    \n",
    "## prediction and test on new data\n",
    "_, test_acc = forward(model, test_X, test_y)\n",
    "print \"accuracy on test data\", test_acc.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
