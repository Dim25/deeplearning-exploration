{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google TensorFlow\n",
    "- Most of notes below are my personal understanding, it may be even less accurate than other notes considering the library is released just recently.\n",
    "- Google has just released its TensorFlow, which looks familiar to existing deep learning frameworks, especially theano, but with a gola of standarizing the inferface of machine learning in the world! It does it by using the same api on different devices.\n",
    "- it generlizes from deep leanring framework to general machine learning tasks (model, objective and optimization)\n",
    "- it wraps up api in a so far the cleanest way\n",
    "- [getting started page](http://tensorflow.org/get_started)\n",
    "\n",
    "### installation\n",
    "```sh\n",
    "# For CPU-only version\n",
    "$ pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n",
    "\n",
    "# For GPU-enabled version (only install this version if you have the CUDA sdk installed)\n",
    "$ pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n",
    "```\n",
    "\n",
    "\n",
    "### Some notes from tensorflow online tutorial\n",
    "- Tensorflow relies on a highly efficient C++ backend to do its computation. The connection to this backend is called a `session`. The common usage for TensorFlow programs is to first create a graph and then launch it in a session.\n",
    "- `InteractiveSession` allows you to interleave operations which build a computation graph with ones that run the graph. This is particularly convenient when working in interactive contexts like iPython. If you are not using an InteractiveSession, then you should build the entire computation graph before starting a session and launching the graph.\n",
    "- The role of the Python code is therefore to build this external computation graph, and to dictate which parts of the computation graph should be run.\n",
    "- placeholder -- a value that we'll input when we ask TensorFlow to run a computation. Its specially useful to minibatch based training. The main diff between placeholder variable and a tf.Variable is that placeholder only needs partial shape information (even that is optional, but useful for debugging), whereas a variable needs both initial value and thus shape information.\n",
    "- A Variable is a value that lives in TensorFlow's computation graph. It can be used and even modified by the computation. In machine learning applications, one generally has the model paramaters be Variables.\n",
    "- ***In otherwords, data are usually represented by numpy arrays. Input/output to tf graph (aka flow of graph) are presented by tf.placeholder, but models are usually represented as tf.Variable. Data can be directly used as inputs to a session graph, if the inputs don't need to change (e.g. whole batch trainig instead of minibatch)***\n",
    "- Before Variables can be used within a session, they must be initialized using that session. This step takes the initial values (in this case tensors full of zeros) that have already been specified, and assigns them to each Variable. \n",
    "- When you execute the `run` of graph session, or any part of it (via a operator, e.g., train, init). you can always use `feed_dict` parameter to replace any tensor in your computation graph -- it's not restricted to just placeholders.\n",
    "- Use `run` on an opeartor (node) in the graph, use `eval` on an variable in the graph, both can take `feed_dict` as parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 100) (100,)\n",
      "0 0.279973\n",
      "40 6.67697e-05\n",
      "80 4.3256e-07\n",
      "120 4.87139e-09\n",
      "160 5.81966e-11\n",
      "200 6.94262e-13\n",
      "best fit: [[ 0.09999834  0.19999768]] [ 0.30000219]\n"
     ]
    }
   ],
   "source": [
    "## fitting a linear model by sgd\n",
    "## data - numpy array\n",
    "## variable - states of the computing graph, use session to access their values\n",
    "## variable values (e.g., initial values) - tensors of tf (flows in the graph)\n",
    "## session - computing graph\n",
    "## drive of training - init and train operations of session (nodes in the graph)\n",
    "\n",
    "\n",
    "## notice the x is the ndim x ninstances shape\n",
    "x_data = np.float32(np.random.rand(2, 100))\n",
    "y_data = np.dot([0.1, 0.2], x_data) + 0.3\n",
    "print x_data.shape, y_data.shape\n",
    "\n",
    "## linear model - variables with init values\n",
    "## init values should be generated using tf functions, with a similiar api to numpy\n",
    "W = tf.Variable(tf.random_uniform([1, 2], -1., 1.))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "## like in theano, combining variable and numpy arrays to create new variable\n",
    "y = tf.matmul(W, x_data) + b\n",
    "\n",
    "## construct objective and training operations (functions)\n",
    "loss = tf.reduce_mean(tf.square(y - y_data)) # variable\n",
    "optimizer = tf.train.GradientDescentOptimizer(.5) # factory\n",
    "train = optimizer.minimize(loss) # operation (function)\n",
    "\n",
    "init = tf.initialize_all_variables() # operation (function)\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in xrange(0, 201):\n",
    "    sess.run(train)\n",
    "    if step % 40 == 0:\n",
    "        print step, sess.run(loss) # use session to access variables\n",
    "        \n",
    "print \"best fit:\", sess.run(W), sess.run(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello MNIST part I - softmax regression\n",
    "- see the notes above on how tensorflow computation graph works, specially the roles of placeholder, ndarray, variables, functions and etc.\n",
    "- use a Interactive session instead of a session\n",
    "- design the one-layer MLP with tensorflow - so that you can interleaving building and running of the graph session.\n",
    "    - use `tf.placeholder` as ***input variables*** (images and targets) to the graph for minibatch training, specifying their partial shapes. They can be thought of input flow to the graph. Compared to variables, their shapes are not fixed.\n",
    "    ```python\n",
    "    x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "    y_ = tf.placeholder(\"float\", shape=[None, 10])\n",
    "    ```\n",
    "    - create ***model variables*** using `tf.Variable` - you don't need to change their shapes during the run time, but their values will be changed during opimization. Make sure to initialize them by their intial values, all at once. `sess.run(tf.initialize_all_variables())`\n",
    "    - create ***other variables***, e.g., preditected target, objective, e.g., \n",
    "    ```python\n",
    "    y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    ```\n",
    "    - create ***operators (computations)*** with `run` in the session, e.g., picking the optimizer, minimize the objective variable `train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)`\n",
    "    - `execute the computation by either operator.run or variable.eval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "onehot = OneHotEncoder(n_values=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (10000, 784) (10000, 784)\n",
      "(50000, 10) (10000, 10) (10000, 10)\n",
      "0.0 0.996094\n",
      "float32 float32\n"
     ]
    }
   ],
   "source": [
    "## load data\n",
    "import cPickle\n",
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = cPickle.load(open(\"../data/mnist.pkl\"))\n",
    "## TensorFlow DOESNT ACCEPT SPARSE MATRIX!\n",
    "train_y = onehot.fit_transform(train_y[..., np.newaxis]).astype(np.float32).toarray()\n",
    "valid_y = onehot.fit_transform(valid_y[..., np.newaxis]).astype(np.float32).toarray()\n",
    "test_y = onehot.fit_transform(test_y[..., np.newaxis]).astype(np.float32).toarray()\n",
    "\n",
    "train_x, train_y = shuffle(train_x, train_y)\n",
    "print train_x.shape, valid_x.shape, test_x.shape\n",
    "print train_y.shape, valid_y.shape, test_y.shape\n",
    "print train_x.min(), train_x.max()\n",
    "print train_x.dtype, train_y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, train_error 2.279591 valid_error 2.29217\n",
      "iteration 1000, train_error 0.612567 valid_error 0.569313\n",
      "iteration 2000, train_error 0.359394 valid_error 0.45055\n",
      "iteration 3000, train_error 0.390407 valid_error 0.403482\n",
      "iteration 4000, train_error 0.280292 valid_error 0.377815\n",
      "iteration 5000, train_error 0.321717 valid_error 0.359983\n",
      "iteration 6000, train_error 0.493204 valid_error 0.347379\n",
      "iteration 7000, train_error 0.324589 valid_error 0.337543\n",
      "iteration 8000, train_error 0.486199 valid_error 0.329372\n",
      "iteration 9000, train_error 0.406300 valid_error 0.32331\n",
      "classification rate for test data: 0.9113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError() in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f577decd790>> ignored\n"
     ]
    }
   ],
   "source": [
    "session = tf.InteractiveSession()\n",
    "## INDEPENDENT VARIABLES - input flow to the graph\n",
    "x = tf.placeholder(\"float32\", shape = (None, 28*28))\n",
    "t = tf.placeholder(\"float32\", shape = (None, 10))\n",
    "\n",
    "## STATE VARIBLES graph states\n",
    "W = tf.Variable(tf.zeros([28*28, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "## DEPENDENT VARIABLES other graph variables\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "p = tf.arg_max(y, dimension = 1)\n",
    "\n",
    "## objective and training method\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(t * tf.log(y), 1)) ## reducer, mean of sum on 2 axes\n",
    "## you need cast boolean to float32 when using tf.mean, .........\n",
    "classification_rate = tf.reduce_mean(tf.cast(tf.equal(p, tf.arg_max(t, dimension = 1)), \"float32\"))\n",
    "train_fn = tf.train.GradientDescentOptimizer(0.01, ).minimize(cross_entropy)\n",
    "\n",
    "## train loop\n",
    "session.run(tf.initialize_all_variables())\n",
    "\n",
    "batch_size = 50\n",
    "for i in xrange(10000):\n",
    "    ii = i % 1000\n",
    "    batch_x = train_x[ii*batch_size:(ii+1)*batch_size, :]\n",
    "    batch_t = train_y[ii*batch_size:(ii+1)*batch_size, :]\n",
    "    batch_x, batch_t = mnist.train.next_batch(batch_size)\n",
    "\n",
    "    train_fn.run(feed_dict = {x: batch_x, t: batch_t}, session = session, )\n",
    "    if i % 1000 == 0:\n",
    "        print (\"iteration %i, train_error %f\" % (i, cross_entropy.eval(feed_dict = {x: batch_x, t: batch_t}))),  \n",
    "        print (\"valid_error %g\" % cross_entropy.eval(feed_dict = {x: valid_x, t: valid_y}))\n",
    "        \n",
    "print \"classification rate for test data: %g\" % classification_rate.eval({x: test_x, t: test_y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello MNIST part II - CNN\n",
    "- One should generally initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients.\n",
    "```python\n",
    "def weight_variable(shape):\n",
    "  ## discard values beyond 2 * std - keep the small values\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "```\n",
    "- Since we're using ReLU neurons, it is also good practice to initialize them with a slightly positive initial bias to avoid \"dead neurons.\"\n",
    "```python\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "```\n",
    "- ***TensorFlow's 3D data flow (image data) are of the shape (ninstances, width, height, nchannels). As such, the strides and kernel sizes parameters in tf.nn.conv2d and tf.nn.max_pool all follow this shape convention. The strides will determine the final resulted convolved image size***\n",
    "- ***the weight of conv layer has the shape [patchwidth, patchheight, nchannels, nfilters] for W and [nfilters] for b.***\n",
    "- For example, for the vanilla of conv-maxpool, (i.e.,  convolutions uses a stride of one and are zero padded so that the output is the same size as the input. pooling is plain old max pooling over 2x2 blocks) can be implemented as \n",
    "\n",
    "```python\n",
    "def conv2d(x, W):\n",
    "  ## the filter size will be determined by w\n",
    "  ## strides are all one in each dimension \n",
    "  ## - so the convolved image has the same size with the original one\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  ## 2x2 kernels, with 2x2 stride (without overlapping)\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "```\n",
    "- the cnn implemented below has the structure (conv, maxpool, conv, maxpool, dense, dropout, softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 10)\n",
      "(10000, 32, 32, 3) (10000, 10)\n",
      "0.0 1.0\n",
      "float32 float32\n"
     ]
    }
   ],
   "source": [
    "## load the cifar data\n",
    "import cPickle\n",
    "def load_cifar_train():\n",
    "    imgpath = \"../data/cifar-10-batches-py/\"\n",
    "    image_batchs, label_batchs = [], []\n",
    "    for f in [\"data_batch_%i\" for i in range(1, 6)]:\n",
    "        batch = cPickle.load(open(\"../data/cifar-10-batches-py/data_batch_1\"))\n",
    "        image_batchs.append(np.array([np.transpose(r.reshape((3, 32, 32)), (1, 2, 0)) for r in batch[\"data\"]]))\n",
    "        label_batchs.append(np.array(batch[\"labels\"]))\n",
    "    images = np.concatenate(image_batchs, axis = 0)\n",
    "    labels = np.concatenate(label_batchs, axis = 0)\n",
    "    return (images, labels)\n",
    "\n",
    "def load_cifar_test():\n",
    "    batch = cPickle.load(open(\"../data/cifar-10-batches-py/test_batch\"))\n",
    "    images = np.array([np.transpose(r.reshape((3, 32, 32)), (1, 2, 0)) for r in batch[\"data\"]])\n",
    "    labels = np.array(batch[\"labels\"])\n",
    "    return (images, labels)\n",
    "\n",
    "cifar_train_images, cifar_train_labels = load_cifar_train()\n",
    "cifar_test_images, cifar_test_labels = load_cifar_test()\n",
    "\n",
    "cifar_train_images = np.multiply(cifar_train_images, 1./255)\n",
    "cifar_test_images = np.multiply(cifar_test_images, 1./255)\n",
    "onehot = OneHotEncoder(10)\n",
    "cifar_train_labels = onehot.fit_transform(cifar_train_labels[..., np.newaxis]).toarray()\n",
    "cifar_test_labels = onehot.fit_transform(cifar_test_labels[..., np.newaxis]).toarray()\n",
    "\n",
    "cifar_train_images = cifar_train_images.astype(np.float32)\n",
    "cifar_train_labels = cifar_train_labels.astype(np.float32)\n",
    "cifar_test_images = cifar_test_images.astype(np.float32)\n",
    "cifar_test_labels = cifar_test_labels.astype(np.float32)\n",
    "\n",
    "print cifar_train_images.shape, cifar_train_labels.shape\n",
    "print cifar_test_images.shape, cifar_test_labels.shape\n",
    "print cifar_train_images.min(), cifar_train_images.max()\n",
    "print cifar_train_images.dtype, cifar_train_labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train error 6.62549 test error 6.46967\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "train error 1.3946 test error 1.57598\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "train error 1.09843 test error 1.40748\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "train error 0.849883 test error 1.29979\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-51304ffc81cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcifar_train_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcifar_train_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     train_step.run(feed_dict={x_image: x_data, y_label: y_data, keep_prob : .5}, \n\u001b[1;32m---> 67\u001b[1;33m                    session = sess)\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m     \"\"\"\n\u001b[1;32m-> 1267\u001b[1;33m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[1;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   2761\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2762\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 2763\u001b[1;33m   \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 404\u001b[1;33m                                target_list)\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## build the network\n",
    "\n",
    "## helper function on creating model variable with proper init values\n",
    "def weight_variable(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "def bias_variable(shape):\n",
    "    return tf.Variable(tf.constant(value = 0.1, shape = shape))\n",
    "\n",
    "## input/output place holders\n",
    "x_image = tf.placeholder(dtype = np.float32, shape = (None, 32, 32, 3))\n",
    "y_label = tf.placeholder(dtype = np.float32, shape = (None, 10))\n",
    "\n",
    "## layer1 conv/maxpool - with relu layer\n",
    "W_conv1 = weight_variable([5, 5, 3, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "## x_image : (None, 32, 32, 3), W_conv1 : (5, 5, 3, 32), generate (?, ?, 3, 32)\n",
    "## strides with all 1s will result in the same image size\n",
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=(1,1,1,1), padding=\"SAME\") \n",
    "                     + b_conv1)\n",
    "h_pool1 = tf.nn.max_pool(h_conv1, ksize = (1, 2, 2, 1), \n",
    "                         strides = (1, 2, 2, 1), padding=\"SAME\")\n",
    "\n",
    "## layer2 conv/maxpool - with relu layer - double filter sizes\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, strides=(1, 1, 1, 1), padding=\"SAME\")\n",
    "                    + b_conv2)\n",
    "h_pool2 = tf.nn.max_pool(h_conv2, ksize = (1, 2, 2, 1), strides=(1, 2, 2, 1), \n",
    "                        padding = \"SAME\")\n",
    "\n",
    "## fully connected dense layer \n",
    "## - now the image flow should have the size (None, 8, 8, 64), with 1024 hidden dimension\n",
    "W_fc1 = weight_variable([8*8*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_reshaped = tf.reshape(h_pool2, (-1, 8*8*64))\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_reshaped, W_fc1) + b_fc1)\n",
    "\n",
    "## now we need regularization - dropout layer\n",
    "## need to control dropout as it is on when training and generally off when predicting\n",
    "## again use a placeholder as a controller\n",
    "keep_prob = tf.placeholder(np.float32) # it is a scalar\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob, )\n",
    "\n",
    "## softmax layer\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y_fc2 = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "## cost function and predicted class labels\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y_label * tf.log(y_fc2), 1))\n",
    "predicted_label = tf.arg_max(y_fc2, dimension=1)\n",
    "match = tf.equal(tf.arg_max(y_label, dimension=1), predicted_label)\n",
    "classification_rate = tf.reduce_mean(tf.cast(match, np.float32))\n",
    "\n",
    "## training function\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "## training loop\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "batch_size = 50\n",
    "for epoch in xrange(20000):\n",
    "    i = epoch % (cifar_train_images.shape[0] / batch_size)\n",
    "    batch = slice(i*batch_size, (i+1)*batch_size)\n",
    "    x_data, y_data = cifar_train_images[batch, ...], cifar_train_labels[batch, ...]\n",
    "    tic = time()\n",
    "    train_step.run(feed_dict={x_image: x_data, y_label: y_data, keep_prob : .5}, \n",
    "                   session = sess)\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        toc = time() - tic\n",
    "        print \"train error\", cross_entropy.eval(feed_dict={x_image:x_data, \n",
    "                                                          y_label:y_data, \n",
    "                                                          keep_prob:1.}, session = sess), \n",
    "        print \"test error\", cross_entropy.eval(feed_dict={x_image:cifar_test_images[:1000,...], \n",
    "                                                          y_label:cifar_test_labels[:1000, ...], \n",
    "                                                          keep_prob:1.}, session = sess),\n",
    "        print 'time (s)', toc\n",
    "        \n",
    "print \"test classification rate: \", classification_rate.eval(feed_dict={x_image:cifar_test_images, \n",
    "                                                          y_label:cifar_test_labels, \n",
    "                                                        keep_prob:1.}, session = sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.574"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_rate.eval(feed_dict={x_image:cifar_test_images[2000:2500, ...], \n",
    "                                                          y_label:cifar_test_labels[2000:2500, ...], \n",
    "                                                                        keep_prob:1.}, session = sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
