{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google TensorFlow\n",
    "- Most of notes below are my personal understanding, it may be even less accurate than other notes considering the library is released just recently.\n",
    "- Google has just released its TensorFlow, which looks familiar to existing deep learning frameworks, especially theano, but with a gola of standarizing the inferface of machine learning in the world! It does it by using the same api on different devices.\n",
    "- it generlizes from deep leanring framework to general machine learning tasks (model, objective and optimization)\n",
    "- it wraps up api in a so far the cleanest way\n",
    "- [getting started page](http://tensorflow.org/get_started)\n",
    "\n",
    "### installation\n",
    "```sh\n",
    "# For CPU-only version\n",
    "$ pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n",
    "\n",
    "# For GPU-enabled version (only install this version if you have the CUDA sdk installed)\n",
    "$ pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n",
    "```\n",
    "\n",
    "\n",
    "### Some notes from tensorflow online tutorial\n",
    "- Tensorflow relies on a highly efficient C++ backend to do its computation. The connection to this backend is called a `session`. The common usage for TensorFlow programs is to first create a graph and then launch it in a session.\n",
    "- `InteractiveSession` allows you to interleave operations which build a computation graph with ones that run the graph. This is particularly convenient when working in interactive contexts like iPython. If you are not using an InteractiveSession, then you should build the entire computation graph before starting a session and launching the graph.\n",
    "- The role of the Python code is therefore to build this external computation graph, and to dictate which parts of the computation graph should be run.\n",
    "- placeholder -- a value that we'll input when we ask TensorFlow to run a computation. Its specially useful to minibatch based training. The main diff between placeholder variable and a tf.Variable is that placeholder only needs partial shape information (even that is optional, but useful for debugging), whereas a variable needs both initial value and thus shape information.\n",
    "- A Variable is a value that lives in TensorFlow's computation graph. It can be used and even modified by the computation. In machine learning applications, one generally has the model paramaters be Variables.\n",
    "- ***In otherwords, data are usually represented by numpy arrays. Input/output to tf graph (aka flow of graph) are presented by tf.placeholder, but models are usually represented as tf.Variable. Data can be directly used as inputs to a session graph, if the inputs don't need to change (e.g. whole batch trainig instead of minibatch)***\n",
    "- Before Variables can be used within a session, they must be initialized using that session. This step takes the initial values (in this case tensors full of zeros) that have already been specified, and assigns them to each Variable. \n",
    "- When you execute the `run` of graph session, or any part of it (via a operator, e.g., train, init). you can always use `feed_dict` parameter to replace any tensor in your computation graph -- it's not restricted to just placeholders.\n",
    "- Use `run` on an opeartor (node) in the graph, use `eval` on an variable in the graph, both can take `feed_dict` as parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 100) (100,)\n",
      "0 0.0830833\n",
      "40 6.37427e-05\n",
      "80 1.3638e-06\n",
      "120 4.43106e-08\n",
      "160 1.44728e-09\n",
      "200 4.72533e-11\n",
      "best fit: [[ 0.10002078  0.200018  ]] [ 0.2999807]\n"
     ]
    }
   ],
   "source": [
    "## fitting a linear model by sgd\n",
    "## data - numpy array\n",
    "## variable - states of the computing graph, use session to access their values\n",
    "## variable values (e.g., initial values) - tensors of tf (flows in the graph)\n",
    "## session - computing graph\n",
    "## drive of training - init and train operations of session (nodes in the graph)\n",
    "\n",
    "\n",
    "## notice the x is the ndim x ninstances shape\n",
    "x_data = np.float32(np.random.rand(2, 100))\n",
    "y_data = np.dot([0.1, 0.2], x_data) + 0.3\n",
    "print x_data.shape, y_data.shape\n",
    "\n",
    "## linear model - variables with init values\n",
    "## init values should be generated using tf functions, with a similiar api to numpy\n",
    "W = tf.Variable(tf.random_uniform([1, 2], -1., 1.))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "## like in theano, combining variable and numpy arrays to create new variable\n",
    "y = tf.matmul(W, x_data) + b\n",
    "\n",
    "## construct objective and training operations (functions)\n",
    "loss = tf.reduce_mean(tf.square(y - y_data)) # variable\n",
    "optimizer = tf.train.GradientDescentOptimizer(.5) # factory\n",
    "train = optimizer.minimize(loss) # operation (function)\n",
    "\n",
    "init = tf.initialize_all_variables() # operation (function)\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in xrange(0, 201):\n",
    "    sess.run(train)\n",
    "    if step % 40 == 0:\n",
    "        print step, sess.run(loss) # use session to access variables\n",
    "        \n",
    "print \"best fit:\", sess.run(W), sess.run(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello MNIST part I - softmax regression\n",
    "- see the notes above on how tensorflow computation graph works, specially the roles of placeholder, ndarray, variables, functions and etc.\n",
    "- use a Interactive session instead of a session\n",
    "- design the one-layer MLP with tensorflow - so that you can interleaving building and running of the graph session.\n",
    "    - use `tf.placeholder` as ***input variables*** (images and targets) to the graph for minibatch training, specifying their partial shapes. They can be thought of input flow to the graph. Compared to variables, their shapes are not fixed.\n",
    "    ```python\n",
    "    x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "    y_ = tf.placeholder(\"float\", shape=[None, 10])\n",
    "    ```\n",
    "    - create ***model variables*** using `tf.Variable` - you don't need to change their shapes during the run time, but their values will be changed during opimization. Make sure to initialize them by their intial values, all at once. `sess.run(tf.initialize_all_variables())`\n",
    "    - create ***other variables***, e.g., preditected target, objective, e.g., \n",
    "    ```python\n",
    "    y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    ```\n",
    "    - create ***operators (computations)*** with `run` in the session, e.g., picking the optimizer, minimize the objective variable `train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)`\n",
    "    - `execute the computation by either operator.run or variable.eval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "onehot = OneHotEncoder(n_values=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (10000, 784) (10000, 784)\n",
      "(50000, 10) (10000, 10) (10000, 10)\n",
      "0.0 0.996094\n",
      "float32 float32\n"
     ]
    }
   ],
   "source": [
    "## load data\n",
    "import cPickle\n",
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = cPickle.load(open(\"../data/mnist.pkl\"))\n",
    "## TensorFlow DOESNT ACCEPT SPARSE MATRIX!\n",
    "train_y = onehot.fit_transform(train_y[..., np.newaxis]).astype(np.float32).toarray()\n",
    "valid_y = onehot.fit_transform(valid_y[..., np.newaxis]).astype(np.float32).toarray()\n",
    "test_y = onehot.fit_transform(test_y[..., np.newaxis]).astype(np.float32).toarray()\n",
    "\n",
    "train_x, train_y = shuffle(train_x, train_y)\n",
    "print train_x.shape, valid_x.shape, test_x.shape\n",
    "print train_y.shape, valid_y.shape, test_y.shape\n",
    "print train_x.min(), train_x.max()\n",
    "print train_x.dtype, train_y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, train_error 2.281228 valid_error 2.29227\n",
      "iteration 1000, train_error 0.832524 valid_error 0.571073\n",
      "iteration 2000, train_error 0.759068 valid_error 0.452804\n",
      "iteration 3000, train_error 0.734087 valid_error 0.406345\n",
      "iteration 4000, train_error 0.720655 valid_error 0.380456\n",
      "iteration 5000, train_error 0.711747 valid_error 0.36354\n",
      "iteration 6000, train_error 0.705170 valid_error 0.351419\n",
      "iteration 7000, train_error 0.700012 valid_error 0.342197\n",
      "iteration 8000, train_error 0.695814 valid_error 0.334879\n",
      "iteration 9000, train_error 0.692311 valid_error 0.328889\n",
      "classification rate for test data: 0.9113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError() in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f4af4025450>> ignored\n"
     ]
    }
   ],
   "source": [
    "session = tf.InteractiveSession()\n",
    "## INDEPENDENT VARIABLES - input flow to the graph\n",
    "x = tf.placeholder(\"float32\", shape = (None, 28*28))\n",
    "t = tf.placeholder(\"float32\", shape = (None, 10))\n",
    "\n",
    "## STATE VARIBLES graph states\n",
    "W = tf.Variable(tf.zeros([28*28, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "## DEPENDENT VARIABLES other graph variables\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "p = tf.arg_max(y, dimension = 1)\n",
    "\n",
    "## objective and training method\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(t * tf.log(y), 1)) ## reducer, mean of sum on 2 axes\n",
    "## you need cast boolean to float32 when using tf.mean, .........\n",
    "classification_rate = tf.reduce_mean(tf.cast(tf.equal(p, tf.arg_max(t, dimension = 1)), \"float32\"))\n",
    "train_fn = tf.train.GradientDescentOptimizer(0.01, ).minimize(cross_entropy)\n",
    "\n",
    "## train loop\n",
    "session.run(tf.initialize_all_variables())\n",
    "\n",
    "batch_size = 50\n",
    "for i in xrange(10000):\n",
    "    ii = i % 1000\n",
    "    batch_x = train_x[ii*batch_size:(ii+1)*batch_size, :]\n",
    "    batch_t = train_y[ii*batch_size:(ii+1)*batch_size, :]\n",
    "    #batch_x, batch_t = mnist.train.next_batch(batch_size)\n",
    "\n",
    "    train_fn.run(feed_dict = {x: batch_x, t: batch_t}, session = session, )\n",
    "    if i % 1000 == 0:\n",
    "        print (\"iteration %i, train_error %f\" % (i, cross_entropy.eval(feed_dict = {x: batch_x, t: batch_t}))),  \n",
    "        print (\"valid_error %g\" % cross_entropy.eval(feed_dict = {x: valid_x, t: valid_y}))\n",
    "        \n",
    "print \"classification rate for test data: %g\" % classification_rate.eval({x: test_x, t: test_y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello MNIST part II - CNN\n",
    "- One should generally initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients.\n",
    "```python\n",
    "def weight_variable(shape):\n",
    "  ## discard values beyond 2 * std - keep the small values\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "```\n",
    "- Since we're using ReLU neurons, it is also good practice to initialize them with a slightly positive initial bias to avoid \"dead neurons.\"\n",
    "```python\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "```\n",
    "- ***TensorFlow's 3D data flow (image data) are of the shape (ninstances, width, height, nchannels). As such, the strides and kernel sizes parameters in tf.nn.conv2d and tf.nn.max_pool all follow this shape convention. The strides will determine the final resulted convolved image size***\n",
    "- ***the weight of conv layer has the shape [patchwidth, patchheight, nchannels, nfilters] for W and [nfilters] for b.***\n",
    "- For example, for the vanilla of conv-maxpool, (i.e.,  convolutions uses a stride of one and are zero padded so that the output is the same size as the input. pooling is plain old max pooling over 2x2 blocks) can be implemented as \n",
    "\n",
    "```python\n",
    "def conv2d(x, W):\n",
    "  ## the filter size will be determined by w\n",
    "  ## strides are all one in each dimension \n",
    "  ## - so the convolved image has the same size with the original one\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  ## 2x2 kernels, with 2x2 stride (without overlapping)\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "```\n",
    "- the cnn implemented below has the structure (conv, maxpool, conv, maxpool, dense, dropout, softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 10)\n",
      "(10000, 32, 32, 3) (10000, 10)\n",
      "0.0 1.0\n",
      "float32 float32\n"
     ]
    }
   ],
   "source": [
    "## load the cifar data\n",
    "import cPickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def load_cifar_train():\n",
    "    imgpath = \"../data/cifar-10-batches-py/\"\n",
    "    image_batchs, label_batchs = [], []\n",
    "    for f in [\"data_batch_%i\" for i in range(1, 6)]:\n",
    "        batch = cPickle.load(open(\"../data/cifar-10-batches-py/data_batch_1\"))\n",
    "        image_batchs.append(np.array([np.transpose(r.reshape((3, 32, 32)), (1, 2, 0)) for r in batch[\"data\"]]))\n",
    "        label_batchs.append(np.array(batch[\"labels\"]))\n",
    "    images = np.concatenate(image_batchs, axis = 0)\n",
    "    labels = np.concatenate(label_batchs, axis = 0)\n",
    "    return (images, labels)\n",
    "\n",
    "def load_cifar_test():\n",
    "    batch = cPickle.load(open(\"../data/cifar-10-batches-py/test_batch\"))\n",
    "    images = np.array([np.transpose(r.reshape((3, 32, 32)), (1, 2, 0)) for r in batch[\"data\"]])\n",
    "    labels = np.array(batch[\"labels\"])\n",
    "    return (images, labels)\n",
    "\n",
    "cifar_train_images, cifar_train_labels = load_cifar_train()\n",
    "cifar_test_images, cifar_test_labels = load_cifar_test()\n",
    "\n",
    "cifar_train_images = np.multiply(cifar_train_images, 1./255)\n",
    "cifar_test_images = np.multiply(cifar_test_images, 1./255)\n",
    "onehot = OneHotEncoder(10)\n",
    "cifar_train_labels = onehot.fit_transform(cifar_train_labels[..., np.newaxis]).toarray()\n",
    "cifar_test_labels = onehot.fit_transform(cifar_test_labels[..., np.newaxis]).toarray()\n",
    "\n",
    "cifar_train_images = cifar_train_images.astype(np.float32)\n",
    "cifar_train_labels = cifar_train_labels.astype(np.float32)\n",
    "cifar_test_images = cifar_test_images.astype(np.float32)\n",
    "cifar_test_labels = cifar_test_labels.astype(np.float32)\n",
    "\n",
    "print cifar_train_images.shape, cifar_train_labels.shape\n",
    "print cifar_test_images.shape, cifar_test_labels.shape\n",
    "print cifar_train_images.min(), cifar_train_images.max()\n",
    "print cifar_train_images.dtype, cifar_train_labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train error 8.16461 time (s) 0.409779787064\n",
      "100 train error 1.83488 time (s) 0.371289968491\n",
      "200 train error 1.59829 time (s) 0.443302154541\n",
      "300 train error 1.74269 time (s) 0.395637989044\n",
      "400 train error 1.51177 time (s) 0.415384054184\n",
      "500 train error 1.52278 time (s) 0.420225858688\n",
      "600 train error 1.42883 time (s) 0.431298971176\n",
      "700 train error 1.40683 time (s) 0.369115114212\n",
      "800 train error 1.33825 time (s) 0.399115085602\n",
      "900 train error 1.34097 time (s) 0.392896175385\n",
      "1000 train error 1.20071 time (s) 0.37681221962\n",
      "1100 train error 1.04692 time (s) 0.380440950394\n",
      "1200 train error 1.20273 time (s) 0.340277910233\n",
      "1300 train error 1.136 time (s) 0.429105043411\n",
      "1400 train error 1.00207 time (s) 0.39781498909\n",
      "1500 train error 0.962834 time (s) 0.387616157532\n",
      "1600 train error 1.08313 time (s) 0.378707885742\n",
      "1700 train error 1.00179 time (s) 0.3853328228\n",
      "1800 train error 0.890499 time (s) 0.385201931\n",
      "1900 train error 0.768781 time (s) 0.390058040619\n",
      "2000 train error 0.829846 time (s) 0.406065940857\n",
      "2100 train error 0.678187 time (s) 0.404422998428\n",
      "2200 train error 0.697197 time (s) 0.405371189117\n",
      "2300 train error 0.668042 time (s) 0.427441120148\n",
      "2400 train error 0.666746 time (s) 0.376621007919\n",
      "2500 train error 0.559779 time (s) 0.428813934326\n",
      "2600 train error 0.606666 time (s) 0.38240981102\n",
      "2700 train error 0.567632 time (s) 0.37758898735\n",
      "2800 train error 0.691015 time (s) 0.400461912155\n",
      "2900 train error 0.478817 time (s) 0.393664836884\n",
      "3000 train error 0.388681 time (s) 0.384763002396\n",
      "3100 train error 0.499053 time (s) 0.408987998962\n",
      "3200 train error 0.454214 time (s) 0.422039985657\n",
      "3300 train error 0.392513 time (s) 0.394022941589\n",
      "3400 train error 0.419335 time (s) 0.398633956909\n",
      "3500 train error 0.334108 time (s) 0.371651887894\n",
      "3600 train error 0.384524 time (s) 0.376392126083\n",
      "3700 train error 0.353987 time (s) 0.39432311058\n",
      "3800 train error 0.285921 time (s) 0.399185180664\n",
      "3900 train error 0.260751 time (s) 0.376941919327\n",
      "4000 train error 0.221763 time (s) 0.37748003006\n",
      "4100 train error 0.292268 time (s) 0.393577098846\n",
      "4200 train error 0.312658 time (s) 0.390041828156\n",
      "4300 train error 0.19622 time (s) 0.38615489006\n",
      "4400 train error 0.241017 time (s) 0.435271024704\n",
      "4500 train error 0.191193 time (s) 0.382207870483\n",
      "4600 train error 0.204736 time (s) 0.407806873322\n",
      "4700 train error 0.159656 time (s) 0.373042821884\n",
      "4800 train error 0.18375 time (s) 0.374913930893\n",
      "4900 train error 0.222469 time (s) 0.365883111954\n",
      "5000 train error 0.142156 time (s) 0.430639028549\n",
      "5100 train error 0.118317 time (s) 0.422394037247\n",
      "5200 train error 0.136777 time (s) 0.411612033844\n",
      "5300 train error 0.12496 time (s) 0.384822130203\n",
      "5400 train error 0.10211 time (s) 0.393831014633\n",
      "5500 train error 0.0935094 time (s) 0.396483898163\n",
      "5600 train error 0.115262 time (s) 0.376486063004\n",
      "5700 train error 0.10351 time (s) 0.397884130478\n",
      "5800 train error 0.0790329 time (s) 0.413871049881\n",
      "5900 train error 0.105837 time (s) 0.387926101685\n",
      "6000 train error 0.0650255 time (s) 0.373298168182\n",
      "6100 train error 0.0657455 time (s) 0.392874956131\n",
      "6200 train error 0.0661533 time (s) 0.395540952682\n",
      "6300 train error 0.0761233 time (s) 0.389773845673\n",
      "6400 train error 0.0458204 time (s) 0.378659963608\n",
      "6500 train error 0.0585181 time (s) 0.383233070374\n",
      "6600 train error 0.0594209 time (s) 0.397867918015\n",
      "6700 train error 0.0999324 time (s) 0.391068935394\n",
      "6800 train error 0.0413706 time (s) 0.38756108284\n",
      "6900 train error 0.0435588 time (s) 0.384319067001\n",
      "7000 train error 0.0410489 time (s) 0.406107187271\n",
      "7100 train error 0.0421312 time (s) 0.37909579277\n",
      "7200 train error 0.0350203 time (s) 0.396157026291\n",
      "7300 train error 0.0578265 time (s) 0.377003192902\n",
      "7400 train error 0.0232801 time (s) 0.373970031738\n",
      "7500 train error 0.0245545 time (s) 0.396901130676\n",
      "7600 train error 0.0306202 time (s) 0.381861925125\n",
      "7700 train error 0.0255974 time (s) 0.395686149597\n",
      "7800 train error 0.0222806 time (s) 0.388800859451\n",
      "7900 train error 0.0314396 time (s) 0.426628828049\n",
      "8000 train error 0.0424291 time (s) 0.38398194313\n",
      "8100 train error 0.0237203 time (s) 0.407710075378\n",
      "8200 train error 0.0163924 time (s) 0.39074587822\n",
      "8300 train error 0.0193286 time (s) 0.391861915588\n",
      "8400 train error 0.015036 time (s) 0.390139102936\n",
      "8500 train error 0.0230133 time (s) 0.39346909523\n",
      "8600 train error 0.0207778 time (s) 0.390064954758\n",
      "8700 train error 0.0195599 time (s) 0.391798973083\n",
      "8800 train error 0.0228659 time (s) 0.401988983154\n",
      "8900 train error 0.0131797 time (s) 0.359981060028\n",
      "9000 train error 0.0132762 time (s) 0.36801815033\n",
      "9100 train error 0.0140481 time (s) 0.505316972733\n",
      "9200 train error 0.0108836 time (s) 0.385244131088\n",
      "9300 train error 0.0112281 time (s) 0.386929035187\n",
      "9400 train error 0.0111447 time (s) 0.418463945389\n",
      "9500 train error 0.0118967 time (s) 0.401937961578\n",
      "9600 train error 0.00687789 time (s) 0.39953994751\n",
      "9700 train error 0.0118371 time (s) 0.386887073517\n",
      "9800 train error 0.0134376 time (s) 0.367290019989\n",
      "9900 train error 0.00603001 time (s) 0.383443117142\n",
      "10000 train error 0.00968481 time (s) 0.401495933533\n",
      "10100 train error 0.00991792 time (s) 0.396259069443\n",
      "10200 train error 0.00951669 time (s) 0.416010141373\n",
      "10300 train error 0.00555941 time (s) 0.398210048676\n",
      "10400 train error 0.00835097 time (s) 0.39071893692\n",
      "10500 train error 0.00983782 time (s) 0.394009113312\n",
      "10600 train error 0.0112935 time (s) 0.410206794739\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-4da5e0b609ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mtic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     train_step.run(feed_dict={x_image: x_data, y_label: y_data, keep_prob : .5}, \n\u001b[1;32m---> 68\u001b[1;33m                    session = sess)\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m     \"\"\"\n\u001b[1;32m-> 1267\u001b[1;33m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[1;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   2761\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2762\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 2763\u001b[1;33m   \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 404\u001b[1;33m                                target_list)\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## build the network\n",
    "\n",
    "## helper function on creating model variable with proper init values\n",
    "def weight_variable(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "def bias_variable(shape):\n",
    "    return tf.Variable(tf.constant(value = 0.1, shape = shape))\n",
    "\n",
    "## input/output place holders\n",
    "x_image = tf.placeholder(dtype = np.float32, shape = (None, 32, 32, 3))\n",
    "y_label = tf.placeholder(dtype = np.float32, shape = (None, 10))\n",
    "\n",
    "## layer1 conv/maxpool - with relu layer\n",
    "W_conv1 = weight_variable([5, 5, 3, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "## x_image : (None, 32, 32, 3), W_conv1 : (5, 5, 3, 32), generate (?, ?, 3, 32)\n",
    "## strides with all 1s will result in the same image size\n",
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=(1,1,1,1), padding=\"SAME\") \n",
    "                     + b_conv1)\n",
    "h_pool1 = tf.nn.max_pool(h_conv1, ksize = (1, 2, 2, 1), \n",
    "                         strides = (1, 2, 2, 1), padding=\"SAME\")\n",
    "\n",
    "## layer2 conv/maxpool - with relu layer - double filter sizes\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, strides=(1, 1, 1, 1), padding=\"SAME\")\n",
    "                    + b_conv2)\n",
    "h_pool2 = tf.nn.max_pool(h_conv2, ksize = (1, 2, 2, 1), strides=(1, 2, 2, 1), \n",
    "                        padding = \"SAME\")\n",
    "\n",
    "## fully connected dense layer \n",
    "## - now the image flow should have the size (None, 8, 8, 64), with 1024 hidden dimension\n",
    "W_fc1 = weight_variable([8*8*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_reshaped = tf.reshape(h_pool2, (-1, 8*8*64))\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_reshaped, W_fc1) + b_fc1)\n",
    "\n",
    "## now we need regularization - dropout layer\n",
    "## need to control dropout as it is on when training and generally off when predicting\n",
    "## again use a placeholder as a controller\n",
    "keep_prob = tf.placeholder(np.float32) # it is a scalar\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob, )\n",
    "\n",
    "## softmax layer\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y_fc2 = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "## cost function and predicted class labels\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y_label * tf.log(y_fc2), 1))\n",
    "predicted_label = tf.arg_max(y_fc2, dimension=1)\n",
    "match = tf.equal(tf.arg_max(y_label, dimension=1), predicted_label)\n",
    "classification_rate = tf.reduce_mean(tf.cast(match, np.float32))\n",
    "\n",
    "## training function\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "## training loop\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "batch_size = 128\n",
    "for epoch in xrange(50000):\n",
    "    i = epoch % (cifar_train_images.shape[0] / batch_size)\n",
    "    batch = slice(i*batch_size, (i+1)*batch_size)\n",
    "    x_data, y_data = cifar_train_images[batch, ...], cifar_train_labels[batch, ...]\n",
    "    tic = time()\n",
    "    train_step.run(feed_dict={x_image: x_data, y_label: y_data, keep_prob : .5}, \n",
    "                   session = sess)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        toc = time() - tic\n",
    "        print epoch, \"train error\", cross_entropy.eval(feed_dict={x_image:x_data, \n",
    "                                                          y_label:y_data, \n",
    "                                                          keep_prob:1.}, session = sess), \n",
    "        print 'time (s)', toc\n",
    "#     if epoch % 1000 == 0:\n",
    "#         print \"test error\", cross_entropy.eval(feed_dict={x_image:cifar_test_images[:1000,...], \n",
    "#                                                           y_label:cifar_test_labels[:1000, ...], \n",
    "#                                                           keep_prob:1.}, session = sess)\n",
    "        \n",
    "        \n",
    "print \"test classification rate: \", classification_rate.eval(feed_dict={x_image:cifar_test_images, \n",
    "                                                          y_label:cifar_test_labels, \n",
    "                                                        keep_prob:1.}, session = sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59240001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_rate.eval(feed_dict={x_image:cifar_test_images, \n",
    "                                                          y_label:cifar_test_labels, \n",
    "                                                                        keep_prob:1.}, session = sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
