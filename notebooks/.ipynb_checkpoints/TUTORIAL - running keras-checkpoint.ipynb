{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Framework based on theano\n",
    "- [github](https://github.com/fchollet/keras)\n",
    "- [website](http://keras.io/)\n",
    "- [examples showing off modelling capability](http://keras.io/examples/)\n",
    "- [example codes on image/text data](https://github.com/fchollet/keras/tree/master/examples)\n",
    "\n",
    "### Philosophy\n",
    "- fast prototyping with flexible and minimal configuration: Torch like interface within Python, also supports sklearn-like prediction interface, e.g., `fit`, `train_on_batch`, `evaluate`, `predict_classes`, `predict_proba`.\n",
    "- run on both cpu and gpu\n",
    "- support both convotlutional networks and recurrent networks\n",
    "- easy to extend\n",
    "\n",
    "### Basic Usage\n",
    "Like almost all other apis, the main compoents of keras are (1) different types of layers (2) model (net) consisting of layers and a loss function, (3) optimizers and optionally some data-processing facilities e.g. for image/text/sequence data. \n",
    "\n",
    "### Main APIS\n",
    "\n",
    "#### A. [Data Processing](http://keras.io/preprocessing/sequence/) - most of them are helper functions, and helper processors\n",
    "- packages: \n",
    "    - `keras.preprocessing.sequence` for sequence data\n",
    "    - `keras.preprocessing.text` for text data\n",
    "    - `keras.preprocessing.image` for image data\n",
    "\n",
    "#### B. [Layers](http://keras.io/layers/core/)\n",
    "- packages:\n",
    "    - `keras.layers.core` for core layers\n",
    "    - `keras.layers.convolutional` for convolution/pooling layers\n",
    "    - `keras.layers.recurrent` for recurrent layers\n",
    "    - `keras.layers.advanced_activations` as its name suggests\n",
    "    - `keras.layers.normalization` for normalizations\n",
    "    - `keras.layers.embeddings` for text embedding (vector representation)\n",
    "    - `keras.layers.noise` for noise-adding\n",
    "    - `keras.layers.containers` for ensemble/composite layers, e.g. sequentially stacked multilayers\n",
    "- activation functions: activations of layers can be specified (1) either via a separate activation layer or (2) through the activation argument supported by all forward layers.Existing activations are\n",
    "    - softmax: expect shape to be either (nsamples, ntimesteps, ndims) or (nsamples, ndims)\n",
    "    - softplus\n",
    "    - relu\n",
    "    - tanh\n",
    "    - sigmoid\n",
    "    - hard_sigmoid\n",
    "    - linear\n",
    "- initialization of layer weights can be specified by `init` param in the layer construtor, out-of-box initialization include\n",
    "    - uniform\n",
    "    - lecun_uniform (uniform initialization scaled by sqrt of nins)\n",
    "    - normal\n",
    "    - identity \n",
    "    - orthogonal\n",
    "    - zero\n",
    "    - glorot_normal (Gaussian initialization scaled by nin+nout)\n",
    "    - glorot_uniform\n",
    "    - he_normal\n",
    "    - he_uniform\n",
    "- regularization of layer weights: they are either on layer weights and/or layer activations. These are done via three parameters to a layer. The parameters can have different regularizer instances from the `keras.regularizers` package.\n",
    "    - `W_regularizer`: l1(l=0.01), l2(l=0.01), l1l2(l1=0.01, l2=0.01)\n",
    "    - `b_regularizer`: l1(l=0.01), l2(l=0.01), l1l2(l1=0.01, l2=0.01)\n",
    "    - `activity_regularizer`: activity_l1(l=0.01), activity_l2(l=0.01), activity_l1l2(l1=0.01, l2=0.01)\n",
    "- constraints: some layers need constraints, see [doc](http://keras.io/constraints/) for details\n",
    "\n",
    "#### C. [Objective Functions](http://keras.io/objectives/)\n",
    "Objective functions can be specifed either by name (see below the out-of-box objective function names) or a Theano symbolic function that returns a scalar for each data point - exmaples can be found in [source code](https://github.com/fchollet/keras/blob/master/keras/objectives.py). Available functions include,\n",
    "- mean_squared_error / mse\n",
    "- mean_absolute_error / mae\n",
    "- mean_absolute_percentage_error / mape\n",
    "- mean_squared_logarithmic_error / msle\n",
    "- squared_hinge: only for binary classification\n",
    "- hinge: only for binary classification\n",
    "- binary_crossentropy: Also known as logloss.\n",
    "- categorical_crossentropy: aka softmax for multi-classification. ***It needs the labels are in one-hot-encoding, i.e., binary arrays of shape (nsamples, nclasses)***\n",
    "\n",
    "Note that keras follows the convention of theano where the final output function (e.g., softmax) is treated as an activation, instead of part of loss function (as in Caffe)\n",
    "\n",
    "#### D. [Optimizers](http://keras.io/optimizers/) \n",
    "Existing optimizers and their parameters can be found in the [doc](http://keras.io/optimizers/).\n",
    "- [comparison of different optimization e.g. rmsprop](http://www.erogol.com/comparison-sgd-vs-momentum-vs-rmsprop-vs-momentumrmsprop/)\n",
    "\n",
    "#### E. [Callback functors](http://keras.io/callbacks/)\n",
    "Callback functors are subclasses of `keras.callbacks.Callback` with specific event slots such as `on_train_begin/end(logs={})`, `on_epoch_begin/end(epoch, logs={})`, `on_batch_begin/end(batch, logs={})`. The commonly used out-of-box callbacks are \n",
    "- `ModelCheckpoint(filepath, verbose = 0, save_best_only=False)`: Save the model after every epoch. If save_best_only=True, the latest best model according to the validation loss will not be overwritten.\n",
    "- `EarlyStopping(monitor='val_loss', patience=0, verbose=0)`: Stop training after no improvement of the metric monitor is seen for patience epochs. The parameter of monitor is a key in the `logs` dictionary passed into event listeners.\n",
    "\n",
    "#### F. [Models](http://keras.io/models/)\n",
    "- it is the main access point for training/evaluating. \n",
    "- it assembles other components such as layers, objective functions and optimizers, e.g.,\n",
    "    - add layer by `model.add`\n",
    "    - set loss function and optimizer in `model.compile`\n",
    "    - set callback functions in `model.fit`\n",
    "- specify callback functions at different stages\n",
    "- typical steps to build a keras model\n",
    "    - [optionally] massage the data into right format via data process helpers\n",
    "    - create a model via constructors (most of time Sequential, sometime Graph)\n",
    "    - create layers and add them to the model by `model.add`\n",
    "    - specify loss function and optimizer by `model.compile`\n",
    "    - [optionally] specify callback functions for house-keeping\n",
    "    - train the model with data by `model.fit` or `model.batch_train`\n",
    "    - evaluate the performance and go back to tune the parameters and models\n",
    "    - make predictions on new data\n",
    "\n",
    "## read and visualize the inner layer of model\n",
    "keras exposes the layers and its parameters, activations via the `.layers` member\n",
    "\n",
    "And that is pretty much everything to know about keras for a good start. It is highly recommended to read its well written [source code](https://github.com/fchollet/keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical Model Structures in keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Quadro K2000\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils ## utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST with vanila MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28) (60000,) (10000,)\n",
      "(60000, 784) (10000, 784)\n",
      "(60000, 10) (10000, 10)\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 0\n",
      "1s - loss: 0.5192 - acc: 0.8477 - val_loss: 0.2308 - val_acc: 0.9329\n",
      "Epoch 1\n",
      "1s - loss: 0.2375 - acc: 0.9299 - val_loss: 0.1678 - val_acc: 0.9488\n",
      "Epoch 2\n",
      "1s - loss: 0.1775 - acc: 0.9462 - val_loss: 0.1555 - val_acc: 0.9510\n",
      "Epoch 3\n",
      "1s - loss: 0.1432 - acc: 0.9568 - val_loss: 0.1222 - val_acc: 0.9630\n",
      "Epoch 4\n",
      "1s - loss: 0.1227 - acc: 0.9626 - val_loss: 0.1137 - val_acc: 0.9651\n",
      "Epoch 5\n",
      "1s - loss: 0.1062 - acc: 0.9682 - val_loss: 0.1069 - val_acc: 0.9670\n",
      "Epoch 6\n",
      "1s - loss: 0.0973 - acc: 0.9701 - val_loss: 0.1003 - val_acc: 0.9698\n",
      "Epoch 7\n",
      "1s - loss: 0.0859 - acc: 0.9738 - val_loss: 0.1012 - val_acc: 0.9695\n",
      "Epoch 8\n",
      "1s - loss: 0.0765 - acc: 0.9759 - val_loss: 0.0995 - val_acc: 0.9704\n",
      "Epoch 9\n",
      "1s - loss: 0.0718 - acc: 0.9776 - val_loss: 0.0947 - val_acc: 0.9731\n",
      "Epoch 10\n",
      "1s - loss: 0.0678 - acc: 0.9786 - val_loss: 0.0950 - val_acc: 0.9731\n",
      "Epoch 11\n",
      "1s - loss: 0.0628 - acc: 0.9805 - val_loss: 0.0948 - val_acc: 0.9740\n",
      "Epoch 12\n",
      "1s - loss: 0.0595 - acc: 0.9818 - val_loss: 0.0990 - val_acc: 0.9721\n",
      "Epoch 13\n",
      "1s - loss: 0.0545 - acc: 0.9828 - val_loss: 0.0923 - val_acc: 0.9739\n",
      "Epoch 14\n",
      "1s - loss: 0.0519 - acc: 0.9836 - val_loss: 0.0987 - val_acc: 0.9731\n",
      "Epoch 15\n",
      "1s - loss: 0.0498 - acc: 0.9840 - val_loss: 0.0922 - val_acc: 0.9741\n",
      "Epoch 16\n",
      "1s - loss: 0.0485 - acc: 0.9846 - val_loss: 0.0960 - val_acc: 0.9747\n",
      "Epoch 17\n",
      "1s - loss: 0.0457 - acc: 0.9849 - val_loss: 0.0955 - val_acc: 0.9743\n",
      "Epoch 18\n",
      "1s - loss: 0.0417 - acc: 0.9862 - val_loss: 0.0987 - val_acc: 0.9753\n",
      "Epoch 19\n",
      "1s - loss: 0.0415 - acc: 0.9863 - val_loss: 0.0934 - val_acc: 0.9747\n",
      "[0.08139571923918193, 0.97789999999999999]\n",
      "10000/10000 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97789999999999999"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load minist raw images \n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "print train_X.shape, test_X.shape, train_y.shape, test_y.shape\n",
    "\n",
    "## massage the data to normalize/vectorize\n",
    "## vectorizing images this way wont assume any spatial information in iamges, contrary to cnn\n",
    "def process_mnist_input(images):\n",
    "    return images.reshape((-1, 28 * 28)).astype(np.float32) / 255.\n",
    "## one-hot encoding of class labels, required by softmax-crossentropy loss\n",
    "def process_mnist_output(labels):\n",
    "    return np_utils.to_categorical(labels, nb_classes=10)\n",
    "train_X, test_X = process_mnist_input(train_X), process_mnist_input(test_X)\n",
    "print train_X.shape, test_X.shape\n",
    "train_y, test_y = process_mnist_output(train_y), process_mnist_output(test_y)\n",
    "print train_y.shape, test_y.shape\n",
    "\n",
    "## build the model - vanila mlp\n",
    "model = Sequential()\n",
    "model.add(Dense(28 * 28, 128, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, 128, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, 10, activation=\"softmax\"))\n",
    "rms = RMSprop()\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = rms)\n",
    "\n",
    "## train model under an sklearn interface\n",
    "## model snapshot callback\n",
    "save_model = ModelCheckpoint(\"../data/tmp/keras_mnist_nlp.h5\")\n",
    "model.fit(train_X, train_y, batch_size=128, nb_epoch=20, \n",
    "          show_accuracy=True, verbose=2, validation_split=0.3, callbacks=[save_model])\n",
    "\n",
    "## evaluate on test data\n",
    "print model.evaluate(test_X, test_y, show_accuracy=True, verbose=0)\n",
    "np.mean(model.predict_classes(test_X) == test_y.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST with cnn - utilizing the spatial correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28) (10000, 1, 28, 28)\n",
      "(60000, 10) (10000, 10)\n",
      "CPU times: user 5.59 s, sys: 88.3 ms, total: 5.68 s\n",
      "Wall time: 5.7 s\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 0\n",
      "104s - loss: 0.6393 - acc: 0.7980 - val_loss: 0.2947 - val_acc: 0.9038\n",
      "Epoch 1\n",
      "103s - loss: 0.1918 - acc: 0.9420 - val_loss: 0.1669 - val_acc: 0.9471\n",
      "Epoch 2\n",
      "105s - loss: 0.1275 - acc: 0.9619 - val_loss: 0.1104 - val_acc: 0.9661\n",
      "Epoch 3\n",
      "107s - loss: 0.1033 - acc: 0.9691 - val_loss: 0.0852 - val_acc: 0.9747\n",
      "Epoch 4\n",
      "107s - loss: 0.0872 - acc: 0.9739 - val_loss: 0.0730 - val_acc: 0.9788\n",
      "Epoch 5\n",
      "107s - loss: 0.0754 - acc: 0.9767 - val_loss: 0.0739 - val_acc: 0.9786\n",
      "Epoch 6\n",
      "107s - loss: 0.0682 - acc: 0.9789 - val_loss: 0.0544 - val_acc: 0.9839\n",
      "Epoch 7\n",
      "107s - loss: 0.0614 - acc: 0.9812 - val_loss: 0.0615 - val_acc: 0.9832\n",
      "Epoch 8\n",
      "107s - loss: 0.0553 - acc: 0.9830 - val_loss: 0.0553 - val_acc: 0.9836\n",
      "Epoch 9\n",
      "106s - loss: 0.0523 - acc: 0.9841 - val_loss: 0.0694 - val_acc: 0.9785\n",
      "10000/10000 [==============================] - 4s     \n",
      "[0.05793085096268915, 0.9819]\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "\n",
    "## load data\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "\n",
    "## massage data for cnn - make gray data into 3D Tensor, normalize, and convert to float32 for gpu\n",
    "## sometime you need to resize, or crop images to the right shape\n",
    "## one-hot-encode labels\n",
    "def process_mnist_input(images):\n",
    "    return images[:, np.newaxis, :, :].astype(np.float32) / 255.\n",
    "def process_mnist_output(labels):\n",
    "    return np_utils.to_categorical(labels)\n",
    "train_X = process_mnist_input(train_X)\n",
    "test_X = process_mnist_input(test_X)\n",
    "print train_X.shape, test_X.shape\n",
    "train_y, test_y = process_mnist_output(train_y), process_mnist_output(test_y)\n",
    "print train_y.shape, test_y.shape\n",
    "\n",
    "## build cnn - the convolution2D layer doesn't need you to specify stride, because it follows best practice\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(nb_filter = 32, stack_size = 1, nb_row = 3, nb_col = 3, \n",
    "                        border_mode=\"full\", activation=\"relu\"))\n",
    "model.add(Convolution2D(nb_filter = 32, stack_size = 32, nb_row = 3, nb_col = 3, \n",
    "                        activation=\"relu\"))\n",
    "model.add(MaxPooling2D(poolsize=(2, 2)))\n",
    "model.add(Dropout(.25))\n",
    "\n",
    "model.add(Flatten()) ## flatten to vectors - from convolution layer to vector layer\n",
    "## 28x28 image after (2, 2)-pooling becomes (14, 14)\n",
    "model.add(Dense(32 * 14 * 14, 128, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, 10, activation=\"softmax\"))\n",
    "\n",
    "## compile with loss function and optimizer\n",
    "## benchmark the time to compile\n",
    "%time model.compile(loss = \"categorical_crossentropy\", optimizer = \"adadelta\")\n",
    "\n",
    "## train the model\n",
    "model.fit(train_X, train_y, batch_size=512, nb_epoch=10, \n",
    "          validation_split=0.3, show_accuracy=True, verbose=2)\n",
    "print model.evaluate(test_X, test_y, show_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
