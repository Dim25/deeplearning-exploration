{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Framework based on theano\n",
    "- [github](https://github.com/fchollet/keras)\n",
    "- [website](http://keras.io/)\n",
    "- [examples showing off modelling capability](http://keras.io/examples/)\n",
    "- [example codes on image/text data](https://github.com/fchollet/keras/tree/master/examples)\n",
    "\n",
    "### Philosophy\n",
    "- fast prototyping with flexible and minimal configuration: Torch like interface within Python, also supports sklearn-like prediction interface, e.g., `fit`, `train_on_batch`, `evaluate`, `predict_classes`, `predict_proba`.\n",
    "- run on both cpu and gpu\n",
    "- support both convotlutional networks and recurrent networks\n",
    "- easy to extend\n",
    "\n",
    "### Basic Usage\n",
    "Like almost all other apis, the main compoents of keras are (1) different types of layers (2) model (net) consisting of layers and a loss function, (3) optimizers and optionally some data-processing facilities e.g. for image/text/sequence data. \n",
    "\n",
    "### Main APIS\n",
    "\n",
    "#### A. [Data Processing](http://keras.io/preprocessing/sequence/) - most of them are helper functions, and helper processors\n",
    "- packages: \n",
    "    - `keras.preprocessing.sequence` for sequence data\n",
    "    - `keras.preprocessing.text` for text data\n",
    "    - `keras.preprocessing.image` for image data\n",
    "\n",
    "#### B. [Layers](http://keras.io/layers/core/)\n",
    "- packages:\n",
    "    - `keras.layers.core` for core layers\n",
    "    - `keras.layers.convolutional` for convolution/pooling layers\n",
    "    - `keras.layers.recurrent` for recurrent layers\n",
    "    - `keras.layers.advanced_activations` as its name suggests\n",
    "    - `keras.layers.normalization` for normalizations\n",
    "    - `keras.layers.embeddings` for text embedding (vector representation)\n",
    "    - `keras.layers.noise` for noise-adding\n",
    "    - `keras.layers.containers` for ensemble/composite layers, e.g. sequentially stacked multilayers\n",
    "- activation functions: activations of layers can be specified (1) either via a separate activation layer or (2) through the activation argument supported by all forward layers.Existing activations are\n",
    "    - softmax: expect shape to be either (nsamples, ntimesteps, ndims) or (nsamples, ndims)\n",
    "    - softplus\n",
    "    - relu\n",
    "    - tanh\n",
    "    - sigmoid\n",
    "    - hard_sigmoid\n",
    "    - linear\n",
    "- initialization of layer weights can be specified by `init` param in the layer construtor, out-of-box initialization include\n",
    "    - uniform\n",
    "    - lecun_uniform (uniform initialization scaled by sqrt of nins)\n",
    "    - normal\n",
    "    - identity \n",
    "    - orthogonal\n",
    "    - zero\n",
    "    - glorot_normal (Gaussian initialization scaled by nin+nout)\n",
    "    - glorot_uniform\n",
    "    - he_normal\n",
    "    - he_uniform\n",
    "- regularization of layer weights: they are either on layer weights and/or layer activations. These are done via three parameters to a layer. The parameters can have different regularizer instances from the `keras.regularizers` package.\n",
    "    - `W_regularizer`: l1(l=0.01), l2(l=0.01), l1l2(l1=0.01, l2=0.01)\n",
    "    - `b_regularizer`: l1(l=0.01), l2(l=0.01), l1l2(l1=0.01, l2=0.01)\n",
    "    - `activity_regularizer`: activity_l1(l=0.01), activity_l2(l=0.01), activity_l1l2(l1=0.01, l2=0.01)\n",
    "- constraints: some layers need constraints, see [doc](http://keras.io/constraints/) for details\n",
    "\n",
    "#### C. [Objective Functions](http://keras.io/objectives/)\n",
    "Objective functions can be specifed either by name (see below the out-of-box objective function names) or a Theano symbolic function that returns a scalar for each data point - exmaples can be found in [source code](https://github.com/fchollet/keras/blob/master/keras/objectives.py). Available functions include,\n",
    "- mean_squared_error / mse\n",
    "- mean_absolute_error / mae\n",
    "- mean_absolute_percentage_error / mape\n",
    "- mean_squared_logarithmic_error / msle\n",
    "- squared_hinge: only for binary classification\n",
    "- hinge: only for binary classification\n",
    "- binary_crossentropy: Also known as logloss.\n",
    "- categorical_crossentropy: aka softmax for multi-classification. ***It needs the labels are in one-hot-encoding, i.e., binary arrays of shape (nsamples, nclasses)***\n",
    "\n",
    "Note that keras follows the convention of theano where the final output function (e.g., softmax) is treated as an activation, instead of part of loss function (as in Caffe)\n",
    "\n",
    "#### D. [Optimizers](http://keras.io/optimizers/) \n",
    "Existing optimizers and their parameters can be found in the [doc](http://keras.io/optimizers/).\n",
    "- [comparison of different optimization e.g. rmsprop](http://www.erogol.com/comparison-sgd-vs-momentum-vs-rmsprop-vs-momentumrmsprop/)\n",
    "\n",
    "#### E. [Callback functors](http://keras.io/callbacks/)\n",
    "Callback functors are subclasses of `keras.callbacks.Callback` with specific event slots such as `on_train_begin/end(logs={})`, `on_epoch_begin/end(epoch, logs={})`, `on_batch_begin/end(batch, logs={})`. The commonly used out-of-box callbacks are \n",
    "- `ModelCheckpoint(filepath, verbose = 0, save_best_only=False)`: Save the model after every epoch. If save_best_only=True, the latest best model according to the validation loss will not be overwritten.\n",
    "- `EarlyStopping(monitor='val_loss', patience=0, verbose=0)`: Stop training after no improvement of the metric monitor is seen for patience epochs. The parameter of monitor is a key in the `logs` dictionary passed into event listeners.\n",
    "\n",
    "#### F. [Models](http://keras.io/models/)\n",
    "- it is the main access point for training/evaluating. \n",
    "- it assembles other components such as layers, objective functions and optimizers, e.g.,\n",
    "    - add layer by `model.add`\n",
    "    - set loss function and optimizer in `model.compile`\n",
    "    - set callback functions in `model.fit`\n",
    "- specify callback functions at different stages\n",
    "- typical steps to build a keras model\n",
    "    - [optionally] massage the data into right format via data process helpers\n",
    "    - create a model via constructors (most of time Sequential, sometime Graph)\n",
    "    - create layers and add them to the model by `model.add`\n",
    "    - specify loss function and optimizer by `model.compile`\n",
    "    - [optionally] specify callback functions for house-keeping\n",
    "    - train the model with data by `model.fit` or `model.batch_train`\n",
    "    - evaluate the performance and go back to tune the parameters and models\n",
    "    - make predictions on new data\n",
    "\n",
    "## read and visualize the inner layer of model\n",
    "keras exposes the layers and its parameters, activations via the `.layers` member\n",
    "\n",
    "And that is pretty much everything to know about keras for a good start. It is highly recommended to read its well written [source code](https://github.com/fchollet/keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Typical Model Structures in keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils ## utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST with vanila MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28) (60000,) (10000,)\n",
      "(60000, 784) (10000, 784)\n",
      "(60000, 10) (10000, 10)\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 0\n",
      "2s - loss: 0.5160 - acc: 0.8490 - val_loss: 0.2357 - val_acc: 0.9284\n",
      "Epoch 1\n",
      "2s - loss: 0.2342 - acc: 0.9300 - val_loss: 0.1697 - val_acc: 0.9491\n",
      "Epoch 2\n",
      "2s - loss: 0.1745 - acc: 0.9480 - val_loss: 0.1301 - val_acc: 0.9604\n",
      "Epoch 3\n",
      "2s - loss: 0.1407 - acc: 0.9577 - val_loss: 0.1228 - val_acc: 0.9639\n",
      "Epoch 4\n",
      "2s - loss: 0.1172 - acc: 0.9637 - val_loss: 0.1122 - val_acc: 0.9666\n",
      "Epoch 5\n",
      "2s - loss: 0.1062 - acc: 0.9674 - val_loss: 0.1049 - val_acc: 0.9684\n",
      "Epoch 6\n",
      "2s - loss: 0.0925 - acc: 0.9715 - val_loss: 0.0964 - val_acc: 0.9717\n",
      "Epoch 7\n",
      "2s - loss: 0.0832 - acc: 0.9742 - val_loss: 0.0997 - val_acc: 0.9713\n",
      "Epoch 8\n",
      "2s - loss: 0.0757 - acc: 0.9778 - val_loss: 0.1000 - val_acc: 0.9724\n",
      "Epoch 9\n",
      "2s - loss: 0.0702 - acc: 0.9781 - val_loss: 0.0940 - val_acc: 0.9733\n",
      "Epoch 10\n",
      "2s - loss: 0.0670 - acc: 0.9786 - val_loss: 0.0906 - val_acc: 0.9749\n",
      "Epoch 11\n",
      "2s - loss: 0.0605 - acc: 0.9806 - val_loss: 0.0955 - val_acc: 0.9735\n",
      "Epoch 12\n",
      "2s - loss: 0.0560 - acc: 0.9828 - val_loss: 0.0946 - val_acc: 0.9747\n",
      "Epoch 13\n",
      "2s - loss: 0.0520 - acc: 0.9829 - val_loss: 0.0932 - val_acc: 0.9756\n",
      "Epoch 14\n",
      "2s - loss: 0.0500 - acc: 0.9830 - val_loss: 0.0934 - val_acc: 0.9756\n",
      "Epoch 15\n",
      "2s - loss: 0.0476 - acc: 0.9845 - val_loss: 0.0974 - val_acc: 0.9739\n",
      "Epoch 16\n",
      "2s - loss: 0.0456 - acc: 0.9852 - val_loss: 0.0978 - val_acc: 0.9757\n",
      "Epoch 17\n",
      "2s - loss: 0.0424 - acc: 0.9863 - val_loss: 0.0956 - val_acc: 0.9759\n",
      "Epoch 18\n",
      "2s - loss: 0.0396 - acc: 0.9866 - val_loss: 0.0957 - val_acc: 0.9766\n",
      "Epoch 19\n",
      "2s - loss: 0.0385 - acc: 0.9875 - val_loss: 0.0953 - val_acc: 0.9773\n",
      "[0.082504479039031992, 0.97940000000000005]\n",
      "10000/10000 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97940000000000005"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load minist raw images \n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "print train_X.shape, test_X.shape, train_y.shape, test_y.shape\n",
    "\n",
    "## massage the data to normalize/vectorize\n",
    "## vectorizing images this way wont assume any spatial information in iamges, contrary to cnn\n",
    "def process_mnist_input(images):\n",
    "    return images.reshape((-1, 28 * 28)).astype(np.float32) / 255.\n",
    "## one-hot encoding of class labels, required by softmax-crossentropy loss\n",
    "def process_mnist_output(labels):\n",
    "    return np_utils.to_categorical(labels, nb_classes=10)\n",
    "train_X, test_X = process_mnist_input(train_X), process_mnist_input(test_X)\n",
    "print train_X.shape, test_X.shape\n",
    "train_y, test_y = process_mnist_output(train_y), process_mnist_output(test_y)\n",
    "print train_y.shape, test_y.shape\n",
    "\n",
    "## build the model - vanila mlp\n",
    "model = Sequential()\n",
    "model.add(Dense(28 * 28, 128, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, 128, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, 10, activation=\"softmax\"))\n",
    "rms = RMSprop()\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = rms)\n",
    "\n",
    "## train model under an sklearn interface\n",
    "## model snapshot callback\n",
    "save_model = ModelCheckpoint(\"../data/tmp/keras_mnist_nlp.h5\")\n",
    "model.fit(train_X, train_y, batch_size=128, nb_epoch=20, \n",
    "          show_accuracy=True, verbose=2, validation_split=0.3, callbacks=[save_model])\n",
    "\n",
    "## evaluate on test data\n",
    "print model.evaluate(test_X, test_y, show_accuracy=True, verbose=0)\n",
    "np.mean(model.predict_classes(test_X) == test_y.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST with cnn - utilizing the spatial correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28) (10000, 1, 28, 28)\n",
      "(60000, 10) (10000, 10)\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 0\n",
      "1067s - loss: 0.2822 - acc: 0.9115 - val_loss: 0.0785 - val_acc: 0.9767\n",
      "Epoch 1\n",
      "1065s - loss: 0.1020 - acc: 0.9698 - val_loss: 0.0541 - val_acc: 0.9835\n",
      "Epoch 2\n",
      "1062s - loss: 0.0764 - acc: 0.9772 - val_loss: 0.0461 - val_acc: 0.9866\n",
      "Epoch 3\n",
      "1063s - loss: 0.0625 - acc: 0.9817 - val_loss: 0.0737 - val_acc: 0.9797\n",
      "Epoch 4\n",
      "1064s - loss: 0.0517 - acc: 0.9841 - val_loss: 0.0498 - val_acc: 0.9860\n",
      "Epoch 5\n",
      "1060s - loss: 0.0487 - acc: 0.9850 - val_loss: 0.0436 - val_acc: 0.9882\n",
      "Epoch 6\n",
      "1066s - loss: 0.0456 - acc: 0.9864 - val_loss: 0.0428 - val_acc: 0.9888\n",
      "Epoch 7\n",
      "1062s - loss: 0.0383 - acc: 0.9882 - val_loss: 0.0454 - val_acc: 0.9887\n",
      "Epoch 8\n",
      "1068s - loss: 0.0354 - acc: 0.9885 - val_loss: 0.0433 - val_acc: 0.9888\n",
      "Epoch 9\n",
      "1062s - loss: 0.0321 - acc: 0.9898 - val_loss: 0.0421 - val_acc: 0.9895\n",
      "10000/10000 [==============================] - 71s    \n",
      "0.0280298104641\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "\n",
    "## load data\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "\n",
    "## massage data for cnn - make gray data into 3D Tensor, normalize, and convert to float32 for gpu\n",
    "## sometime you need to resize, or crop images to the right shape\n",
    "## one-hot-encode labels\n",
    "def process_mnist_input(images):\n",
    "    return images[:, np.newaxis, :, :].astype(np.float32) / 255.\n",
    "def process_mnist_output(labels):\n",
    "    return np_utils.to_categorical(labels)\n",
    "train_X = process_mnist_input(train_X)\n",
    "test_X = process_mnist_input(test_X)\n",
    "print train_X.shape, test_X.shape\n",
    "train_y, test_y = process_mnist_output(train_y), process_mnist_output(test_y)\n",
    "print train_y.shape, test_y.shape\n",
    "\n",
    "## build cnn - the convolution2D layer doesn't need you to specify stride, because it follows best practice\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(nb_filter = 32, stack_size = 1, nb_row = 3, nb_col = 3, \n",
    "                        border_mode=\"full\", activation=\"relu\"))\n",
    "model.add(Convolution2D(nb_filter = 32, stack_size = 32, nb_row = 3, nb_col = 3, \n",
    "                        activation=\"relu\"))\n",
    "model.add(MaxPooling2D(poolsize=(2, 2)))\n",
    "model.add(Dropout(.25))\n",
    "\n",
    "model.add(Flatten()) ## flatten to vectors - from convolution layer to vector layer\n",
    "## 28x28 image after (2, 2)-pooling becomes (14, 14)\n",
    "model.add(Dense(32 * 14 * 14, 128, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, 10, activation=\"softmax\"))\n",
    "\n",
    "## compile with loss function and optimizer\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adadelta\")\n",
    "\n",
    "## train the model\n",
    "model.fit(train_X, train_y, batch_size=100, nb_epoch=10, \n",
    "          validation_split=0.3, show_accuracy=True, verbose=2)\n",
    "print model.evaluate(test_X, test_y, show_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 71s    \n",
      "[0.028029810464144322, 0.99129999999999996]\n"
     ]
    }
   ],
   "source": [
    "print model.evaluate(test_X, test_y, show_accuracy=True)\n",
    "#print np.mean(model.predict_classes(test_X) == test_y.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
