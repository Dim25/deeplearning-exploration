{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word embedding hand made\n",
    "- CBOW (contexts -> center word) or Skip-gram (center word -> contexts)\n",
    "- negative sampling\n",
    "\n",
    "\n",
    "The implemenation if mainly for demostration and understanding. It is slow and not taking a lot of advantages of GPU - word by word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "import theano\n",
    "from theano import shared, function\n",
    "floatX = theano.config.floatX\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theano.config.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207 10001\n"
     ]
    }
   ],
   "source": [
    "## read data, take 80000 common words\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "text8 = open(\"../data/text8\").read()\n",
    "words = re.compile(\"\\w+\").findall(text8)\n",
    "common_words, _ = zip(*Counter(words).most_common(10000))\n",
    "common_words = set(common_words)\n",
    "words = [w if w in common_words else \"UNKNOWN\" for w in words]\n",
    "\n",
    "vocab = list(set(words))\n",
    "ind2word = dict([(i,w) for i,w in enumerate(vocab)])\n",
    "word2ind = dict([(w,i) for i,w in enumerate(vocab)])\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "data = map(word2ind.get, words)\n",
    "\n",
    "print len(words), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## unigram sample frequences = 3/4 power of raw frequences\n",
    "from collections import Counter\n",
    "wc = Counter(words)\n",
    "N = len(words)\n",
    "sample_probs = np.array([wc[ind2word[i]]\n",
    "                         for i in xrange(len(vocab))])\n",
    "sample_probs = np.power(sample_probs, 3./4)\n",
    "sample_probs = sample_probs / sample_probs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## sampling methods for continous bag of words (CBOW) and skip-gram\n",
    "def sample(data, sample_probs, vocab_size, window_size):\n",
    "    N = len(data)\n",
    "    iword = window_size ## one side window size\n",
    "    while True:\n",
    "        w = data[iword]\n",
    "        positives = np.array(data[iword-window_size:iword] \n",
    "                     + data[iword+1:iword+window_size+1])\n",
    "        negatives = np.random.choice(vocab_size, 2*window_size, \n",
    "                                     p = sample_probs, replace = False)\n",
    "        yield (w, positives, negatives)  \n",
    "        iword += 1\n",
    "        if iword + window_size >= N:\n",
    "            iword = window_size\n",
    "            \n",
    "def random_sample(data, sample_probs, vocab_size, window_size):\n",
    "    N = len(data)\n",
    "    \n",
    "    while True:\n",
    "        iword = np.random.randint(window_size, N-window_size-1) ## one side window size\n",
    "        w = data[iword]\n",
    "        positives = np.array(data[iword-window_size:iword] \n",
    "                     + data[iword+1:iword+window_size+1])\n",
    "        negatives = np.random.choice(vocab_size, 2*window_size, \n",
    "                                     p = sample_probs, replace = False)\n",
    "        yield (w, positives, negatives)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#floatX = \"float64\" ## CPU\n",
    "floatX = theano.config.floatX\n",
    "\n",
    "class CBOW(object):\n",
    "    def __init__(self, window_size, word_dim, lr):\n",
    "        self.window_size = window_size\n",
    "        self.word_dim = word_dim\n",
    "        self.lr = lr ## learning rate\n",
    "        \n",
    "        ## center word matrix\n",
    "        U = shared(np.random.randn(vocab_size, word_dim).astype(floatX) \n",
    "                   / np.sqrt(vocab_size), name = \"U\")\n",
    "        ## context word matrix\n",
    "        V = shared(np.random.randn(vocab_size, word_dim).astype(floatX) \n",
    "                   / np.sqrt(vocab_size), name = \"V\")\n",
    "        \n",
    "        ## inputs\n",
    "        word = T.lscalar(name = \"word\")\n",
    "        positives = T.lvector(name = \"positives\")\n",
    "        negatives = T.lvector(name = \"negatives\")\n",
    "        \n",
    "        positive_scores = V[positives, :].dot(U[word, :].T).flatten()\n",
    "        positive_probs = 1. / (1. + T.exp(-positive_scores))\n",
    "        negative_scores = -V[negatives, :].dot(U[word, :].T).flatten()\n",
    "        negative_probs = 1. / (1. + T.exp(-negative_scores))\n",
    "        \n",
    "        data_loss = -T.log(positive_probs).mean() -T.log(negative_probs).mean()\n",
    "        loss = data_loss ## ignore regularization for simiplicty\n",
    "        \n",
    "        dU = T.grad(loss, U)\n",
    "        dV = T.grad(loss, V)\n",
    "        \n",
    "        self.train = function(inputs = [word, positives, negatives], \n",
    "                              outputs = loss, \n",
    "                              updates = [(U, U-lr*dU)\n",
    "                                        , (V, V-lr*dV)])\n",
    "        self.get_word_vectors = function(inputs = [], outputs = [U, V])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "word_dim = 100\n",
    "lr = 0.1\n",
    "\n",
    "\n",
    "cbow = CBOW(window_size, word_dim, lr)\n",
    "print cbow.train(word = 0, positives = range(1, 6), negatives=range(1, 6)), -np.log(.5) * 2 # for both positive and negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test with initialization\n",
      "0 0 0\n",
      "0 50000 1.22511088363\n",
      "0 100000 1.22330685316\n",
      "0 150000 1.22278726044\n",
      "0 200000 1.22251286374\n",
      "0 250000 1.22094391867\n",
      "0 300000 1.2214876091\n",
      "0 350000 1.219557284\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-249-bda1a5c1c6f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0miword\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositives\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegatives\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositives\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegatives\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dola/workspace/vm/dl/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sampler = random_sample(data, sample_probs, vocab_size, window_size)\n",
    "\n",
    "print \"test with initialization\"\n",
    "\n",
    "iword = 0\n",
    "total_loss = 0\n",
    "after_n_words = 50000\n",
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "    if iword + window_size >= len(data):\n",
    "        iteration += 1\n",
    "        iword = 0\n",
    "    if iword % after_n_words == 0:\n",
    "        print iteration, iword, total_loss / after_n_words\n",
    "        total_loss = 0\n",
    "    iword += 1\n",
    "    word, positives, negatives = sampler.next()\n",
    "    loss = cbow.train(word, positives, negatives)\n",
    "    total_loss += loss\n",
    "    if iteration >= 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "word_vecs, context_vecs = cbow.get_word_vectors()\n",
    "word_vecs = np.asarray(word_vecs)\n",
    "normalized_word_vecs = normalize(word_vecs, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['october', 'july', 'august', 'december', 'april', 'six', 'isbn', 'seven', 'eight', 'nine']\n"
     ]
    }
   ],
   "source": [
    "v = normalized_word_vecs[word2ind[\"nine\"]]\n",
    "i = normalized_word_vecs.dot(v).argsort()[-10:]\n",
    "print map(ind2word.get, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['duke', 'son', 'charles', 'henry', 'pope', 'mary', 'prince', 'iii', 'emperor', 'king']\n"
     ]
    }
   ],
   "source": [
    "v = normalized_word_vecs[word2ind[\"king\"]]\n",
    "i = normalized_word_vecs.dot(v).argsort()[-10:]\n",
    "print map(ind2word.get, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wales', 'academy', 'irish', 'revolution', 'poland', 'league', 'spain', 'scotland', 'founded', 'london']\n"
     ]
    }
   ],
   "source": [
    "v = normalized_word_vecs[word2ind[\"london\"]]\n",
    "i = normalized_word_vecs.dot(v).argsort()[-10:]\n",
    "print map(ind2word.get, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thomas', 'davis', 'friedrich', 'iv', 'champion', 'ca', 'johnson', 'rd', 'architect', 'duke']\n"
     ]
    }
   ],
   "source": [
    "v1 = normalized_word_vecs[word2ind[\"king\"]]\n",
    "v2 = normalized_word_vecs[word2ind[\"man\"]]\n",
    "v3 = normalized_word_vecs[word2ind[\"woman\"]]\n",
    "i = normalized_word_vecs.dot(v1-v2+v3).argsort()[-10:]\n",
    "print map(ind2word.get, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so good because the training is not sufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
